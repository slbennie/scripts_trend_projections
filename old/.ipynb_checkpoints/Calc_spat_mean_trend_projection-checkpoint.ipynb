{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1addd5f-6e2a-43fa-82f8-b155037c3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cftime\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import linregress\n",
    "from eofs.xarray import Eof\n",
    "from eofs.examples import example_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe5a59a-c91a-4956-9ee3-e202c8f4e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the correct calendar (model dependent.)\n",
    "def get_time_bounds(calendar_type, start, end):\n",
    "    #1850-2015 all of 2014 - none of 2015.\n",
    "    if calendar_type == cftime.DatetimeNoLeap:\n",
    "        return cftime.DatetimeNoLeap(start,1,16), cftime.DatetimeNoLeap(end,1,16)\n",
    "    elif calendar_type == cftime.Datetime360Day:\n",
    "        return cftime.Datetime360Day(start,1,16), cftime.Datetime360Day(end-1,12,16)\n",
    "    else:\n",
    "        return datetime(start,1,16), datetime(end,1,16)\n",
    "\n",
    "#finding all the models that have ensembles for that experiment.\n",
    "def get_models_for_experiment(experiment):\n",
    "    if experiment == 'historical':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','FGOALS-g3','GISS-E2-1-G','HadGEM3-GC31-LL','IPSL-CM6A-LR','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-aer':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','CNRM-CM6-1','FGOALS-g3','GISS-E2-1-G','HadGEM3-GC31-LL','IPSL-CM6A-LR','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-GHG':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','CNRM-CM6-1','FGOALS-g3','GISS-E2-1-G','HadGEM3-GC31-LL','IPSL-CM6A-LR','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-sol':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','GISS-E2-1-G','HadGEM3-GC31-LL','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-totalO3':\n",
    "        model = ['CanESM5','GISS-E2-1-G','HadGEM3-GC31-LL','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-volc':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','GISS-E2-1-G','HadGEM3-GC31-LL','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "        \n",
    "    return model\n",
    "\n",
    "#Cropping CVDP data to the North Atlantic sector - requires some shifting of 0 of the lat lon coordinate system.\n",
    "def CVDP_EM_crop_NA_sector(filename, pattern):\n",
    "    #function which will crop the historical ensemble mean CVDP output to the NA sector\n",
    "    ds = xr.open_dataset(filename)\n",
    "    ds = ds[pattern]\n",
    "    \n",
    "    #finding the longitudes that are greater than 180\n",
    "    new_lon = np.where(ds.lon > 179, ds.lon -360, ds.lon)\n",
    "    \n",
    "    #creating a copy of the data array where the longitudes have been shifted\n",
    "    ds_shifted = ds.copy()\n",
    "    ds_shifted.coords['lon'] = new_lon\n",
    "    \n",
    "    #Now need to make sure they are in the correct order and then re-index to make sure the lon get put to match the sorted lon\n",
    "    sorted_lon = np.sort(ds_shifted.lon)\n",
    "    ds_shifted = ds_shifted.sel(lon=sorted_lon)\n",
    "    \n",
    "    historical_NAO_EM_shifted = ds_shifted.sel(lat=slice(20,80), lon=slice(-90,40))\n",
    "\n",
    "    return historical_NAO_EM_shifted\n",
    "\n",
    "#Crops to the North Atlantic sector - for the LESFMIP data NOT processed by CVDP\n",
    "def open_cropNA_unitshPA(filename):\n",
    "    #function to crop an ensemble member to the north atlantic region\n",
    "    data = xr.open_dataset(filename)\n",
    "    data_NA = data.sel(lat=slice(20,80), lon=slice(-90,40))/100\n",
    "\n",
    "    return data_NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc9b16f-c17e-4c4c-be0f-e752d43f386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the seasonal and annual ensemble spatial means.\n",
    "\n",
    "def calculate_spatial_ensemble_mean(file_paths, output_file, variable):\n",
    "    #Will be passing through an experiment's model's ensembles.\n",
    "    #opens all the files given by filepath (basically opens all the ensembles)\n",
    "    ds = xr.open_mfdataset(file_paths, combine='nested', concat_dim='ensemble')\n",
    "\n",
    "    #calculate the mean\n",
    "    mean = ds[variable].mean(dim='ensemble')\n",
    "\n",
    "    #save the ensemble mean to the a .nc file\n",
    "    mean.to_netcdf(output_file)\n",
    "    print('saved')\n",
    "\n",
    "    ds.close()\n",
    "    return mean\n",
    "\n",
    "def calculate_seasonal_spatial_ensemble_mean_djf(file_path, var, seas, output_file, year_init, year_final):\n",
    "    #opening dataset\n",
    "    print('in function')\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    #checking it is a datetime object\n",
    "    ds['time'] = xr.decode_cf(ds).time\n",
    "\n",
    "    calendar = type(ds.time.values[0])\n",
    "    \n",
    "    start,end = get_time_bounds(calendar, year_init, year_final)\n",
    "\n",
    "    #selecting the psl variable within time bounds\n",
    "    variable = ds[var].sel(time=slice(start, end))\n",
    "    \n",
    "    #Filter for the desired season (e.g., DJF)\n",
    "    season_mask = variable.time.dt.season == seas\n",
    "    ds_months_seas = variable.sel(time=season_mask)\n",
    "    \n",
    "    #assign and adjust year (DJF split over two years so increasing the year of december and then grouping and finding the mean)\n",
    "    ds_months_seas = ds_months_seas.assign_coords(year=ds_months_seas['time'].dt.year)\n",
    "    ds_months_seas['year'] = ds_months_seas['year'].where(ds_months_seas['time'].dt.month != 12, ds_months_seas['year'] + 1)\n",
    "    #ds_months_seas = ds_months_seas.set_coords('year')\n",
    "    \n",
    "    # average over DJF months for each year\n",
    "    ds_season = ds_months_seas.groupby('year').mean(dim='time')\n",
    "    ds_season.to_netcdf(output_file)\n",
    "    print('saved file')\n",
    "    return ds_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae7eb3a-3a16-4297-8245-09a2195c96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined for calculating the linear trend\n",
    "def calculate_linear_trend_spat_pattern(file_path, variable, output_file):\n",
    "    # Open dataset and extract variable\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    da = ds[variable]\n",
    "\n",
    "    time = ds['year'].values\n",
    "    lat = ds['lat'].values\n",
    "    lon = ds['lon'].values\n",
    "    time_numeric = np.arange(len(time))\n",
    "\n",
    "    slope = np.full((len(lat), len(lon)), np.nan)\n",
    "    intercept = np.full((len(lat), len(lon)), np.nan)\n",
    "    p_value = np.full((len(lat), len(lon)), np.nan)\n",
    "    stderr = np.full((len(lat), len(lon)), np.nan)\n",
    "\n",
    "    for i in range(len(lat)):\n",
    "        for j in range(len(lon)):\n",
    "            ts = da[:, i, j].values\n",
    "            if np.all(np.isfinite(ts)):\n",
    "                reg = linregress(time_numeric, ts)\n",
    "                slope[i, j] = reg.slope\n",
    "                intercept[i, j] = reg.intercept\n",
    "                p_value[i, j] = reg.pvalue\n",
    "                stderr[i, j] = reg.stderr\n",
    "\n",
    "    from scipy.stats import t\n",
    "    n = len(time_numeric)\n",
    "    df = n - 2\n",
    "    alpha = 0.05\n",
    "    t_crit = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "    ci_lower = slope - t_crit * stderr\n",
    "    ci_upper = slope + t_crit * stderr\n",
    "\n",
    "    slope_da = xr.DataArray(slope, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope\")\n",
    "    intercept_da = xr.DataArray(intercept, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"intercept\")\n",
    "    p_value_da = xr.DataArray(p_value, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"p_value\")\n",
    "    ci_lower_da = xr.DataArray(ci_lower, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope_CI_lower\")\n",
    "    ci_upper_da = xr.DataArray(ci_upper, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope_CI_upper\")\n",
    "\n",
    "    # Save to one combined netCDF file\n",
    "    combined_ds = xr.Dataset({\n",
    "        \"slope\": slope_da,\n",
    "        \"intercept\": intercept_da,\n",
    "        \"p_value\": p_value_da,\n",
    "        \"slope_CI_lower\": ci_lower_da,\n",
    "        \"slope_CI_upper\": ci_upper_da\n",
    "    })\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8661f5d2-4c85-48be-9828-8494f9fc6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the regression map\n",
    "def project_trend_onto_regression_map(anomalies, trend, mode, trend_var, e, m, period):\n",
    "    #this will project a trend onto the regression map.\n",
    "    #psl anomalies are linearly regressed onto the PC timeseries (the amount that the EOF's amplitude changes with time)\n",
    "\n",
    "    #setting up output files paths for the projection and the residual\n",
    "    output_regression_map = '/gws/nopw/j04/extant/users/slbennie/regression_patterns/'+mode+'/psl_mon_'+e+'_'+m+'_DJF_'+mode+'_regression_map_'+period+'.nc'\n",
    "\n",
    "\n",
    "    #selecting the mode of which EOF to calculate\n",
    "    if mode == 'NAO':\n",
    "        mode_number = 0\n",
    "    elif mode == 'EA':\n",
    "        mode_number = 1\n",
    "\n",
    "    #opening up all anomaly files and cropping to NA and converting into hPa (anomaly data is in Pa)\n",
    "    anomaly_list = [open_cropNA_unitshPA(f) for f in anomalies]\n",
    "\n",
    "    #selecting the psl data and concatenating list of data arrays.\n",
    "    all_anomalies = xr.concat(anomaly_list, dim='ensemble')['psl']  # Shape: (ensemble, time, lat, lon)\n",
    "    print(all_anomalies.dims)\n",
    "        \n",
    "    #Flatten ensemble and year into one time dimension (needs to be called time for the pcs function to work later)\n",
    "    all_anomalies_stacked = all_anomalies.stack(time=('ensemble', 'year'))\n",
    "    all_anomalies_stacked = all_anomalies_stacked.reset_index('time', drop=True)\n",
    "    all_anomalies_stacked = all_anomalies_stacked.transpose('time', 'lat', 'lon')\n",
    "\n",
    "    #basically weighting so that each grid cell has influence actually proportional to its area\n",
    "    coslat = np.cos(np.deg2rad(all_anomalies_stacked.coords['lat'].values)).clip(0., 1.)\n",
    "    wgts = np.sqrt(coslat)[..., np.newaxis]\n",
    "\n",
    "    #EOF solver\n",
    "    solver = Eof(all_anomalies_stacked, weights=wgts)\n",
    "    \n",
    "    #finding the pattern of the EOF - unitless\n",
    "    EOF_pattern = solver.eofs(neofs=mode_number+1).sel(mode=mode_number)\n",
    "        \n",
    "    #getting the EA Pattern's PC\n",
    "    #using pcscaling=1 for a normalised PC. If not normalised need to divide by the variance of PC ((pc.std(dim='time'))**2) to find the regression map.\n",
    "    pc = solver.pcs(npcs=mode_number+1, pcscaling=1).sel(mode=mode_number)\n",
    "        \n",
    "    #finding regression_map = pattern of psl anomalies regressed onto EA PC, kinda which bits of the trend link to this pattern, units of hPa/unit of PC\n",
    "    #how psl anomalies change spatially for a one-unit change in the PC\n",
    "    regression_map = (all_anomalies_stacked * pc).mean(dim='time')\n",
    "\n",
    "    #making sure that the patterns match what they should for the NAO and EA patterns (basically fixing for sign conventions to make sure physical)\n",
    "    if mode == 'NAO' and regression_map.sel(lat=50, lon=-30, method='nearest') < 0:\n",
    "        regression_map *= -1\n",
    "        pc *= -1\n",
    "\n",
    "    if mode == 'NAO' and regression_map.sel(lat=50, lon=-25, method='nearest') > 0:\n",
    "        regression_map *= -1\n",
    "        pc *= -1\n",
    "        \n",
    "    #outputting .nc files for plotting\n",
    "\n",
    "    \n",
    "    regression_map.name = 'regression_'+mode+'_djf'\n",
    "    regression_map.to_netcdf(output_regression_map)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f147d4f4-8bd0-47c8-ad9f-eba97e905596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined for calculating the projections\n",
    "def project_onto_regression(trend_raw, regression_map, trend_var, mode, e, m, period):\n",
    "    #function which will project a trend (lat,lon) in hPa onto a spatial pattern (lat,lon) hPa to get a single NAO index value\n",
    "    #will then calculate the residual (trend - mode_congruent part) and saves both the NAO congruent part and the residual\n",
    "    #in the same folder, output_file is for the NAO/EA_congruent part filename\n",
    "    #can then change the input spat pattern to calculate the projection onto other eofs, e.g. the EAP\n",
    "\n",
    "\n",
    "    if isinstance(trend_raw, xr.DataArray):\n",
    "        trend = trend_raw\n",
    "    else:\n",
    "        print('here')\n",
    "        trend = trend_raw[trend_var]\n",
    "        \n",
    "    # Weight psl data by coslat to account for grid cell area decreasing with latitude\n",
    "    weights = np.cos(np.radians(trend[\"lat\"].values))\n",
    "    weights_2d = weights[:, np.newaxis]\n",
    "\n",
    "    # weight psl (or another variable) anomalies by area of each gridcell\n",
    "    weighted_trend = trend * weights_2d\n",
    "    weighted_regression = regression_map * weights_2d\n",
    "\n",
    "    # flatten both of the fields so that they are both 1D\n",
    "    trend_flat = weighted_trend.stack(spatial=('lat','lon'))\n",
    "    regression_flat = weighted_regression.stack(spatial=('lat','lon'))\n",
    "\n",
    "    #replace any NaNs with zeros to stop any weird stuff happening\n",
    "    trend_flat = trend_flat.fillna(0)\n",
    "    regression_flat = regression_flat.fillna(0)\n",
    "\n",
    "    #Now do the dot product which is the projection\n",
    "    dot_product = (trend_flat * regression_flat).sum().item()\n",
    "\n",
    "    #calculating the index - or I guess the PC?????\n",
    "    index = dot_product / (regression_flat**2).sum().item()\n",
    "\n",
    "    #Now multiplying the pattern by the index and returning that too\n",
    "    projection = index * regression_map\n",
    "    residual = trend - projection\n",
    "    \n",
    "    projection.name = 'projection_'+mode+'_djf'\n",
    "    residual.name = 'residual_'+mode+'_djf'\n",
    "\n",
    "    output_projection = '/gws/nopw/j04/extant/users/slbennie/projection_indicies/NAtlantic_forced_trends/'+e+'/'+m+'/psl_mon_'+e+'_'+m+'_DJF_'+mode+'_projection_'+period+'.nc'\n",
    "    output_residual = '/gws/nopw/j04/extant/users/slbennie/projection_indicies/NAtlantic_forced_trends/'+e+'/'+m+'/psl_mon_'+e+'_'+m+'_DJF_'+mode+'_residual_'+period+'.nc'\n",
    "    \n",
    "    #outputting .nc files for plotting\n",
    "    projection.to_netcdf(output_projection)\n",
    "    residual.to_netcdf(output_residual)\n",
    "    \n",
    "    return projection, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a243433-6e38-4a71-8428-7076cf92c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the experiments and choose a single mode? could just go ahead and calculate for all of them\n",
    "#trend over the whole period?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b19a671-b5a6-4c36-a85a-5de458e5a9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the spatial ensemble means and trends for experiment e: hist-aer\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n",
      "saved\n",
      "Calculating the seasonal DJF spatial ensemble mean\n",
      "in function\n",
      "saved file\n",
      "Calculating the spatial ensemble means and trends for experiment e: hist-GHG\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n",
      "saved\n",
      "Calculating the seasonal DJF spatial ensemble mean\n",
      "in function\n",
      "saved file\n",
      "Calculating the spatial ensemble means and trends for experiment e: hist-sol\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n",
      "saved\n",
      "Calculating the seasonal DJF spatial ensemble mean\n",
      "in function\n",
      "saved file\n",
      "Calculating the spatial ensemble means and trends for experiment e: hist-totalO3\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#find spatial mean across the ensembles\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculating the ensemble mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mcalculate_spatial_ensemble_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#To use once spatial ensemble mean is calculated\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculating the seasonal \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mseas\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m spatial ensemble mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mcalculate_spatial_ensemble_mean\u001b[0;34m(file_paths, output_file, variable)\u001b[0m\n\u001b[1;32m      9\u001b[0m mean \u001b[38;5;241m=\u001b[39m ds[variable]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#save the ensemble mean to the a .nc file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m ds\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/core/dataarray.py:4139\u001b[0m, in \u001b[0;36mDataArray.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   4135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4136\u001b[0m     \u001b[38;5;66;03m# No problems with the name - so we're fine!\u001b[39;00m\n\u001b[1;32m   4137\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dataset()\n\u001b[0;32m-> 4139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   4140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4147\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4150\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/api.py:1369\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multifile:\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m writer, store\n\u001b[0;32m-> 1369\u001b[0m writes \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, BytesIO):\n\u001b[1;32m   1372\u001b[0m     store\u001b[38;5;241m.\u001b[39msync()\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/common.py:267\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[0;34m(self, compute, chunkmanager_store_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunkmanager_store_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     chunkmanager_store_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 267\u001b[0m delayed_store \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchunkmanager_store_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msources \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/namedarray/daskmanager.py:249\u001b[0m, in \u001b[0;36mDaskManager.store\u001b[0;34m(self, sources, targets, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore\u001b[39m(\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    243\u001b[0m     sources: Any \u001b[38;5;241m|\u001b[39m Sequence[Any],\n\u001b[1;32m    244\u001b[0m     targets: Any,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m store\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/array/core.py:1229\u001b[0m, in \u001b[0;36mstore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute:\n\u001b[1;32m   1228\u001b[0m     store_dsk \u001b[38;5;241m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[0;32m-> 1229\u001b[0m     \u001b[43mcompute_as_if_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mArray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_dsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/base.py:403\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[0;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m schedule \u001b[38;5;241m=\u001b[39m get_scheduler(scheduler\u001b[38;5;241m=\u001b[39mscheduler, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, get\u001b[38;5;241m=\u001b[39mget)\n\u001b[1;32m    402\u001b[0m dsk2 \u001b[38;5;241m=\u001b[39m optimization_function(\u001b[38;5;28mcls\u001b[39m)(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/threaded.py:90\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     88\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 90\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/local.py:501\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    500\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult():\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m    503\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/local.py:138\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#notes find a way to update period so its nicer.\n",
    "#also could fix folder so that I use like a home dir etc.\n",
    "#anomalies have already been calculated separetly for each model so preusming they are all correct this should run?...\n",
    "home = '/gws/nopw/j04/extant/users/slbennie/'\n",
    "\n",
    "\n",
    "variable = 'psl'\n",
    "period = '1850-2015'\n",
    "experiment = ['hist-aer','hist-GHG','hist-sol','hist-totalO3','hist-volc']\n",
    "model = ['CanESM5']\n",
    "modes = ['NAO', 'EA']\n",
    "seas = 'DJF'\n",
    "\n",
    "for e in experiment:\n",
    "    print('Calculating the spatial ensemble means and trends for experiment e:', e)\n",
    "    for m in model:\n",
    "        print('Model: ',m)\n",
    "        #getting the LESFMIP file paths\n",
    "        folder_path = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/'+e+'/'+m+'/'\n",
    "        file_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if e in filename and m in filename]\n",
    "\n",
    "        #creating output files for the mean across ensembles and the djf mean\n",
    "        output_file = home+'ens_mean_spat/psl/'+e+'/'+m+'/'+variable+'_mon_'+e+'_'+m+'_spatial_EM.nc'\n",
    "        output_file_djf = output_file.replace('spatial_EM', 'spatial_'+seas+'_EM_'+period)\n",
    "        \n",
    "        #find spatial mean across the ensembles\n",
    "        print('Calculating the ensemble mean')\n",
    "        calculate_spatial_ensemble_mean(file_paths, output_file, variable)\n",
    "\n",
    "        #To use once spatial ensemble mean is calculated\n",
    "        print('Calculating the seasonal '+seas+' spatial ensemble mean')\n",
    "        calculate_seasonal_spatial_ensemble_mean_djf(output_file, variable, seas, output_file_djf, 1850, 2015)\n",
    "\n",
    "        #find the file for the ensemble spatial mean djf\n",
    "        folder_path = home+'ens_mean_spat/psl/'+e+'/'+m+'/'\n",
    "        file_path = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if seas in filename and period in filename][0]\n",
    "        output_file = home+'trend_calc_LESFMIP/linear_regression/NAO/'+e+'/'+m+'/'+variable+'_mon_'+e+'_'+m+'_'+seas+'_linear_trend_'+period+'_stats.nc'\n",
    "    \n",
    "        calculate_linear_trend_spat_pattern(file_path, variable, output_file)\n",
    "\n",
    "#now creating the NAO adn EA regression maps\n",
    "for m in model:\n",
    "    #find the trend and crop to NA Sector, sort units.\n",
    "    trend_filename = home+'trend_calc_LESFMIP/linear_regression/NAO/historical/'+m+'/'+variable+'_mon_historical_'+m+'_'+seas+'_linear_trend_'+period+'_stats.nc'\n",
    "    trend = open_cropNA_unitshPA(trend_filename) * 165 #converts from Pa/year to hPa (the function converts from Pa to hPa so need to multiply by 165 for the hPa not hPa/year)\n",
    "\n",
    "    #path to the folder containing each experiment and model's psl anomalies (calculated seperatley)\n",
    "    folder_path = f'{home}psl_anomalies/{e}/{m}/'\n",
    "\n",
    "    #creating the list of files for this experiment and model's psl anomalies\n",
    "    ens_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if m in filename and '_EM' not in filename]\n",
    "\n",
    "    project_trend_onto_regression_map(ens_files, trend, 'NAO', 'slope', e, m, period)\n",
    "    project_trend_onto_regression_map(ens_files, trend, 'EA', 'slope', e, m, period)\n",
    "    \n",
    "for m in model:\n",
    "    print('Model:', m)\n",
    "    regression_NAO = xr.open_dataset(home+'regression_patterns/NAO/'+variable+'_mon_historical_'+m+'_'+seas+'_NAO_regression_map_'+period+'.nc')\n",
    "    regression_EA = xr.open_dataset(home+'regression_patterns/EA/'+variable+'_mon_historical_'+m+'_'+seas+'_EA_regression_map_'+period+'.nc')\n",
    "\n",
    "    for e in experiment:\n",
    "        print(\"Calculating the projection of the forced trend onto historical regression map calculated from all ensembles:\", e)\n",
    "        \n",
    "        #now setting up the folder path to get the file names for each experiment's model's forced response's trend\n",
    "        folder_path = home+'trend_calc_LESFMIP/linear_regression/NAO/'+e+'/'+m+'/'\n",
    "            \n",
    "        #Getting the list of file names within the models folder, should only be one trend per model (working off the ensemble means for each model)\n",
    "        ens_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if m in filename and period in filename]\n",
    "            \n",
    "        for i in range(0,len(ens_files)):\n",
    "            #cropping each trend to just the NA sector and whichever time\n",
    "            trend = open_cropNA_unitshPA(ens_files[i])#, 1850,2014)\n",
    "    \n",
    "            #multiplying the trend by 165 to convert to units of hPa (currently in units of hPa/year, trend calculated between 1850-2015)\n",
    "            trend = trend * 165\n",
    "\n",
    "            #calling the projection functions\n",
    "            proj_NAO, residual_NAO = project_onto_regression(trend, regression_NAO['regression_NAO_djf'], 'slope', 'NAO', e, m, period)\n",
    "            proj_EA, residual_EA = project_onto_regression(residual_NAO, regression_EA['regression_EA_djf'], 'slope', 'EA', e, m, period)\n",
    "\n",
    "#print(residual_NAO, residual_EA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbf5ad-fd17-4a66-9cef-a716a0c0f2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
