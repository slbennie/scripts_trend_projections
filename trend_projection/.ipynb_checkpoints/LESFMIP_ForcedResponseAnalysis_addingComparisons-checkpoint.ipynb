{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5efa2fbc-fdcb-415b-a4db-31207da0302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import cftime\n",
    "from datetime import datetime\n",
    "from dask.diagnostics import ProgressBar\n",
    "from scipy.stats import t\n",
    "from scipy.stats import linregress\n",
    "from eofs.xarray import Eof\n",
    "from eofs.examples import example_data_path\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.path as mpath\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patches as mpatches\n",
    "from types import SimpleNamespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfe70287-dfc8-430f-8b95-1d76956ee036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleMemberCalculations:\n",
    "    def __init__(self, data_path: Path, model=None):\n",
    "        #use self. to access attributes\n",
    "        self.data_path = Path(data_path)\n",
    "        \n",
    "        #referencing the parent model and setting up shortcuts\n",
    "        self.model = model\n",
    "        self.experiment = model.experiment if model else None\n",
    "        self.all_experiments = self.experiment.all_experiments if self.experiment else None\n",
    "\n",
    "\n",
    "        #extracting the member_id (ensemble number) from the filename as named in CMIP6 convention\n",
    "        match = re.search(r\"(r\\d+i\\d+p\\d+f\\d+)\", self.data_path.stem)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not extract the member_id from filenmae: {self.data_path.name}\")\n",
    "\n",
    "        #selecting the ensemble number and assigning it to the member ID\n",
    "        self.member_id = match.group(1)\n",
    "\n",
    "\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self):\n",
    "        #loads the data of an ensemble member\n",
    "        self.data = xr.open_dataset(self.data_path)\n",
    "        return self.data\n",
    "\n",
    "    def get_calendar_type(self):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model is not set\")\n",
    "        return self.model.calendar_type\n",
    "\n",
    "    def get_time_bounds(self):\n",
    "        \"\"\"\n",
    "        Access the model's time_bounds - each model has a different calendar thus type of time to index/slice by\n",
    "        \"\"\"\n",
    "        if self.model is None or self.model.time_bounds is None:\n",
    "            raise RuntimeError(\"Model or time_bounds is not set\")\n",
    "        return self.model.time_bounds\n",
    "\n",
    "    def select_data(self, varname: str):\n",
    "        \"\"\"\n",
    "        Just selecting some data from the time bounds in model\n",
    "        \"\"\"\n",
    "        ds = self.load_data()\n",
    "        start, end, delta = self.model.time_bounds\n",
    "        ds = ds[varname].sel(time=slice(start, end))\n",
    "        \n",
    "        return ds\n",
    "\n",
    "    def crop_to_domain_ensemble(self, da: xr.DataArray):\n",
    "        \"\"\"\n",
    "        Crops the ensemble members data (anomaly, raw data etc.) to the lat and lon of the domain\n",
    "        lat_bounds = (lat_min, lat_max)\n",
    "        lon_bounds = (lon_min, lon_max)\n",
    "        Assuming data of the format -180 to 180, -90 to 90???\n",
    "        Will add another function to check and if not flag error\n",
    "        Will be a bit for if model is era5 shift the coords at somepoint in the processing data stages.\n",
    "        Need to already have selected the variable\"\"\"\n",
    "\n",
    "        if self.all_experiments and self.all_experiments.lat_bounds and self.all_experiments.lon_bounds:\n",
    "            da = da.sel(\n",
    "                lat=slice(self.all_experiments.lat_bounds[0], self.all_experiments.lat_bounds[1]),\n",
    "                lon = slice(self.all_experiments.lon_bounds[0], self.all_experiments.lon_bounds[1])\n",
    "            )\n",
    "        return da\n",
    "\n",
    "    def calc_anomaly(self, output_file: Path, varname: str):\n",
    "        \"\"\"\n",
    "        Will calculate the anomaly\n",
    "        \"\"\"\n",
    "        #print(output_file)\n",
    "        #print(self.model.name)\n",
    "        if output_file.exists():\n",
    "            #print(f\"Loading the exisiting file for the anomaly from: {output_file}\")\n",
    "            anomaly = xr.open_dataset(output_file)[varname]\n",
    "            anomaly_cropped = self.crop_to_domain_ensemble(anomaly)\n",
    "            return anomaly_cropped\n",
    "\n",
    "   # def cal_seasonal_mean(self)\n",
    "\n",
    "        \n",
    "\n",
    "class ModelCalculations:\n",
    "    def __init__(self, folder: Path, experiment):\n",
    "        self.folder = Path(folder)\n",
    "        self.experiment = experiment\n",
    "        self.all_experiments = self.experiment.all_experiments if self.experiment else None\n",
    "\n",
    "        #can now set the model name using the folder name\n",
    "        #the .name takes the last bit of the filepath (which in this case is the model)\n",
    "        self.name = self.folder.name\n",
    "        \n",
    "        #creating my dictionary of ensemble members\n",
    "        self.members = {}\n",
    "        for i, file in enumerate(self.folder.glob(\"*.nc\")):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            member = EnsembleMemberCalculations(file, model=self)\n",
    "            self.members[member.member_id] = member\n",
    "        \n",
    "        #basically I pick the first member (load in just one)\n",
    "        #doing this to then use to find the calendar type\n",
    "        #and then find the time bounds as this will be the same time (correct datetime) per model\n",
    "        member1 = next(iter(self.members.values()))\n",
    "        member1.load_data()\n",
    "\n",
    "        #find the calendar type\n",
    "        time_var = member1.data[\"time\"]\n",
    "        self.calendar_type = type(time_var.time.values[0])\n",
    "        self.time_bounds = None\n",
    "\n",
    "    def set_time_bounds(self, start_year: int, end_year: int):\n",
    "        #selecting the correct datetime format for the chosen time period\n",
    "        if issubclass(self.calendar_type, cftime.DatetimeNoLeap):\n",
    "            start = cftime.DatetimeNoLeap(start_year,1,16)\n",
    "            end = cftime.DatetimeNoLeap(end_year,12,16)\n",
    "        elif issubclass(self.calendar_type, cftime.Datetime360Day):\n",
    "            start = cftime.Datetime360Day(start_year,1,16)\n",
    "            end = cftime.Datetime360Day(end_year,12,16)\n",
    "        else:\n",
    "            start = datetime(start_year,1,16)\n",
    "            end = datetime(end_year,12,16)\n",
    "\n",
    "        #assumes that the last year is filled with data\n",
    "        delta = end.year - start.year + 1\n",
    "\n",
    "        self.time_bounds = (start, end, delta)\n",
    "        return self.time_bounds\n",
    "\n",
    "    def crop_to_domain_model(self, da: xr.DataArray):\n",
    "        \"\"\"\n",
    "        Crops the ensemble members data (anomaly, raw data etc.) to the lat and lon of the domain\n",
    "        lat_bounds = (lat_min, lat_max)\n",
    "        lon_bounds = (lon_min, lon_max)\n",
    "        Assuming data of the format -180 to 180, -90 to 90???\n",
    "        Will add another function to check and if not flag error\n",
    "        Will be a bit for if model is era5 shift the coords at somepoint in the processing data stages.\n",
    "        Need to already have selected the variable\"\"\"\n",
    "\n",
    "        if self.all_experiments and self.all_experiments.lat_bounds and self.all_experiments.lon_bounds:\n",
    "            da = da.sel(\n",
    "                lat=slice(self.all_experiments.lat_bounds[0], self.all_experiments.lat_bounds[1]),\n",
    "                lon = slice(self.all_experiments.lon_bounds[0], self.all_experiments.lon_bounds[1])\n",
    "            )\n",
    "        return da\n",
    "\n",
    "\n",
    "    def calc_ensemble_mean(self, varname: str):\n",
    "        \"\"\"\n",
    "        Calculate the SPATIAL ensemble mean for a variable\n",
    "        Restrict it to the time bounds\n",
    "        Could in future restrict to an area\n",
    "        If the file exists load this instead\n",
    "        \"\"\"\n",
    "        output_file = self.all_experiments.output_dir / f\"ens_mean_spat/{model_name}/{varname}_mon_{self.name}_{model_name}_spatial_DJF_EM_1850-2015.nc\"\n",
    "        \n",
    "        if output_file.exists():\n",
    "            print(f\"Loading the exisiting file for the spatial ensemble mean from: {output_file}\")\n",
    "            ens_mean = xr.open_dataset(output_file)[varname]\n",
    "            return ens_mean\n",
    "        \n",
    "        #Get all the filepaths from the ensemble members\n",
    "        #this line basically is getting the filepaths of the ensemble member class objects - i think?\n",
    "        file_paths = [member.data_path for member in self.members.values()]\n",
    "\n",
    "        #then opening them all together\n",
    "        ds = xr.open_mfdataset(\n",
    "            file_paths,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=\"member\",\n",
    "            parallel=True,\n",
    "            chunks={\"member\": 1}\n",
    "        )[varname]\n",
    "\n",
    "        #selecting the time period\n",
    "        start,end, delta = self.time_bounds\n",
    "        ds = ds.sel(time=slice(start,end))\n",
    "\n",
    "        #calculating the ensemble mean across the members\n",
    "        ens_mean = ds.mean(dim=\"member\")\n",
    "    \n",
    "        # Compute the result with a progress bar\n",
    "        print(f\"Computing the ensemble mean for {self.name}...\")\n",
    "        with ProgressBar():\n",
    "            ens_mean.compute()#.to_netcdf(output_file)\n",
    "\n",
    "        ens_mean.to_netcdf(output_file)\n",
    "        print(f'Ensemble mean NOT saved to {output_file}')\n",
    "\n",
    "        return ens_mean\n",
    "\n",
    "    def calc_seasonal_mean_per_model(self, varname:str):\n",
    "        \"\"\"\n",
    "        Calculating the seasonal ensemble mean (could actually just be the seasonal mean for ay single file!!!)\n",
    "        Only accpeting one file so per model\n",
    "        Will save the seasonal mean as well as outputting it\n",
    "        Wil crop to the already saved time periods.\n",
    "        Will at some point make more versatille to choose the months of the year etc. as a list of int.\n",
    "        \"\"\"\n",
    "\n",
    "        #select the time bounds and seasons\n",
    "        start,end = self.time_bounds\n",
    "        print(start,end)\n",
    "        months = self.all_experiments.season\n",
    "        season = \"\".join([calendar.month_abbr[m][0] for m in months])\n",
    "        print(months, season)\n",
    "        \n",
    "        seas_EM_output_file = self.all_experiments.output_dir / f\"ens_mean_spat/{self.experiment.name}/{self.name}/psl_mon_{self.experiment.name}_{self.name}_spatial_{season}_EM_1850-2015.nc\"\n",
    "        \n",
    "        if seas_EM_output_file.exists():\n",
    "            print(f\"Loading the seasonal mean for model: {self.name}\")\n",
    "            seasonal_mean = xr.open_dataset(seas_EM_output_file)[varname]\n",
    "            return seasonal_mean\n",
    "\n",
    "        #loading the ensemble mean\n",
    "        ens_mean = self.calc_ensemble_mean(varname)\n",
    "\n",
    "        #checking it is a datetime object - already done???\n",
    "        #ens_mean['time'] = xr.decode(ens_mean).time\n",
    "\n",
    "        #selecting the ensemble mean for the specified time (might load full ensemble mean as this is already calculated and then slice here)\n",
    "        ens_mean = ens_mean.sel(time=slice(start,end))\n",
    "\n",
    "        #creating mask for the season and grouping the months\n",
    "        mask = ens_mean['time'].dt.month.isin(months)\n",
    "        months_in_seas = ens_mean.sel(time=mask)\n",
    "\n",
    "        #assigning a 'season year'\n",
    "        season_year = months_in_seas['time'].dt.year\n",
    "\n",
    "        #general code for if season WRAPS then fix the year to avg over\n",
    "        if months[0] > months[-1]:\n",
    "            season_year = xr.where(months_in_seas['time'].dt.month >= months[0],\n",
    "                                  season_year +1,\n",
    "                                  season_year)\n",
    "\n",
    "        #assign as \"year\" instead of \"season_year\" for future bits\n",
    "        months_in_seas = months_in_seas.assign_coords(year=season_year)\n",
    "        \n",
    "        #now groupby and average over the years\n",
    "        ens_mean_seas = months_in_seas.groupby('year').mean(dim='time')\n",
    "        #ens_mean_seas.to_netcdf(seas_EM_output_file)\n",
    "\n",
    "        print('calc seas_mean')\n",
    "        \n",
    "        return ens_mean_seas\n",
    "\n",
    "    def calc_linear_trend_per_model(self, varname:str):\n",
    "        \"\"\"\n",
    "        Calcualte the linear trend from the seasonal ensemble mean\n",
    "        Full stats file\n",
    "        Converts from per unit time to just the change over the entire period (delta)\"\"\"\n",
    "        #getting the timebounds\n",
    "        start,end, delta = self.time_bounds\n",
    "             \n",
    "        #create and check if output_file exists\n",
    "        output_file = self.all_experiments.output_dir / f\"trend_calc_LESFMIP/linear_regression/NAO/{self.experiment.name}/{self.name}/psl_mon_{self.experiment.name}_{self.name}_DJF_linear_trend_1850-2015_stats.nc\"\n",
    "\n",
    "        if output_file.exists():\n",
    "            print(f\"loading the linear trend for model: {self.name}\")\n",
    "            return xr.open_dataset(output_file)\n",
    "            \n",
    "        #call the seasonal ensemble mean method\n",
    "        seas_ens_mean = self.calc_seasonal_mean_per_model(varname)\n",
    "        print(seas_ens_mean)\n",
    "    \n",
    "        time = seas_ens_mean['year'].values\n",
    "        lat = seas_ens_mean['lat'].values\n",
    "        lon = seas_ens_mean['lon'].values\n",
    "        time_numeric = np.arange(len(time))\n",
    "    \n",
    "        slope = np.full((len(lat), len(lon)), np.nan)\n",
    "        intercept = np.full((len(lat), len(lon)), np.nan)\n",
    "        p_value = np.full((len(lat), len(lon)), np.nan)\n",
    "        stderr = np.full((len(lat), len(lon)), np.nan)\n",
    "\n",
    "        #now solving for the slope and other stats (multiplying through by delta to convert from per index\n",
    "        #(year) to change over the entire period)\n",
    "        for i in range(len(lat)):\n",
    "            for j in range(len(lon)):\n",
    "                ts = seas_ens_mean[:, i, j].values\n",
    "                if np.all(np.isfinite(ts)):\n",
    "                    reg = linregress(time_numeric, ts)\n",
    "                    slope[i, j] = reg.slope\n",
    "                    intercept[i, j] = reg.intercept\n",
    "                    p_value[i, j] = reg.pvalue\n",
    "                    stderr[i, j] = reg.stderr\n",
    "    \n",
    "        n = len(time_numeric)\n",
    "        df = n - 2\n",
    "        alpha = 0.05\n",
    "        t_crit = t.ppf(1 - alpha/2, df)\n",
    "    \n",
    "        ci_lower = slope - t_crit * stderr\n",
    "        ci_upper = slope + t_crit * stderr\n",
    "    \n",
    "        slope_da = xr.DataArray(slope, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope\")\n",
    "        intercept_da = xr.DataArray(intercept, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"intercept\")\n",
    "        p_value_da = xr.DataArray(p_value, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"p_value\")\n",
    "        ci_lower_da = xr.DataArray(ci_lower, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope_CI_lower\")\n",
    "        ci_upper_da = xr.DataArray(ci_upper, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope_CI_upper\")\n",
    "    \n",
    "        # Save to one combined netCDF file\n",
    "        trend_stats = xr.Dataset({\n",
    "            \"slope\": slope_da,\n",
    "            \"intercept\": intercept_da,\n",
    "            \"p_value\": p_value_da,\n",
    "            \"slope_CI_lower\": ci_lower_da,\n",
    "            \"slope_CI_upper\": ci_upper_da\n",
    "        })\n",
    "        \n",
    "        #combined_ds.to_netcdf(output_file)\n",
    "        return trend_stats\n",
    "\n",
    "    def calc_anomalies_all_members(self, varname: str,):\n",
    "        \"\"\"\n",
    "        This will calculate the anomaly across the ensemble members\n",
    "        Could maybe do this for all the other individual ensemble calcs needed to calc anomaly?\n",
    "        Will return a dictionary {member_id: anomaly_dataarray}\n",
    "        Only calculates for the hisotircal??? - no need it for all!\"\"\"\n",
    "\n",
    "        self.all_experiments.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        results = {}\n",
    "        for member_id, member in self.members.items():\n",
    "            output_file = self.all_experiments.output_dir / f\"psl_anomalies/{self.experiment.name}/{self.name}/psl_mon_{self.experiment.name}_{self.name}_{member_id}_DJF_anomaly.nc\"\n",
    "            anomaly = member.calc_anomaly(output_file, varname)\n",
    "            results[member_id] = anomaly\n",
    "        return results\n",
    "\n",
    "    def calc_EOF_concat(self, varname: str, max_modes: int):\n",
    "        \"\"\"\n",
    "        Calculates the EOF from a concatenated list of anomalies for one model\n",
    "        outputs the number of modes specified\n",
    "        Calls on the anomalies calculated for each ensemble member\n",
    "        This right now does all models not just historical - need an option for that somewhere???\n",
    "        \"\"\"\n",
    "        output_file = self.all_experiments.output_dir / f\"EOF/{self.experiment.name}/DJF/{self.name}/psl_mon_{self.experiment.name}_{self.name}_DJF_EOF_concat_1850-2015.nc\"\n",
    "\n",
    "        if output_file.exists():\n",
    "            #print(f\"Loading the exisiting file for the EOF for: {output_file}\")\n",
    "            EOF = xr.open_dataset(output_file)['eofs']#['__xarray_dataarray_variable__']\n",
    "            #EOF.sel(mode=0).plot()\n",
    "            #plt.show()\n",
    "            return EOF\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('concatenating')\n",
    "            #preprocess anomalies method called\n",
    "            #converting from Pa to hPa - bear in mind I will need to have this as an option to convert.\n",
    "            anomalies = {k: v / 100 for k, v in self.calc_anomalies_all_members(varname).items()}\n",
    "            #anomalies = self.calc_anomalies_all_members(varname)\n",
    "            \n",
    "            anomaly_list = list(anomalies.values())\n",
    "            anomaly_concat = xr.concat(anomaly_list, dim=\"ensemble\")\n",
    "            anomaly_2d = anomaly_concat.stack(time=('ensemble', 'year')).reset_index('time', drop=True)\n",
    "            anomaly_trans = anomaly_2d.transpose('time', 'lat', 'lon')\n",
    "            \n",
    "            coslat = np.cos(np.deg2rad(anomaly_trans.coords['lat'].values)).clip(0., 1.)\n",
    "            wgts = np.sqrt(coslat)[...,np.newaxis]\n",
    "\n",
    "            solver = Eof(anomaly_trans, weights=wgts)\n",
    "            EOF = solver.eofs(neofs=max_modes).sel(mode=([0,max_modes-1]))\n",
    "\n",
    "            #checking orthogonality\n",
    "            #1. Get the first two PCs (time series of each mode)\n",
    "            pcs = solver.pcs(npcs=max_modes, pcscaling=0)  # shape (time, mode)\n",
    "    \n",
    "            # 2. Compute their correlation / covariance\n",
    "            pc_corr = np.corrcoef(pcs[:, 0], pcs[:, 1])[0, 1]\n",
    "            print(\"Correlation between PC1 and PC2:\", pc_corr)\n",
    "\n",
    "            # 3. (Optional) Gram matrix of PCs\n",
    "            G = np.dot(pcs.T, pcs)  # shape (2,2)\n",
    "            print(\"Gram matrix of PCs:\\n\", G)\n",
    "\n",
    "            #Check orthogonality\n",
    "            EOF1 = EOF.sel(mode=0)\n",
    "            EOF2 = EOF.sel(mode=1)\n",
    "    \n",
    "            inner = (EOF1 * EOF2).sum(dim=(\"lat\", \"lon\"))\n",
    "            norm1 = np.sqrt((EOF1**2).sum(dim=(\"lat\", \"lon\")))\n",
    "            norm2 = np.sqrt((EOF2**2).sum(dim=(\"lat\", \"lon\")))\n",
    "            cos_sim = inner / (norm1 * norm2)\n",
    "            is_ortho = bool(np.isclose(inner, 0, atol=1e-10))\n",
    "            \n",
    "            print(f\"Inner product: {inner:.3e}, Cosine similarity: {cos_sim:.3e}\")\n",
    "            if not is_ortho:\n",
    "                print(\"⚠️ Warning: EOF1 and EOF2 are not orthogonal within tolerance!\")\n",
    "\n",
    "            # --- Regression maps for all modes ---\n",
    "            pcs_all = solver.pcs(npcs=max_modes, pcscaling=0)  # shape (time, max_modes)\n",
    "            regression_maps = []\n",
    "        \n",
    "            for mode_idx in range(max_modes):\n",
    "                pc = pcs_all[:, mode_idx]\n",
    "                pc_da = xr.DataArray(pc, dims=\"time\", coords={\"time\": anomaly_trans.coords[\"time\"]})\n",
    "                reg_map = (anomaly_trans * pc_da).mean(dim=\"time\") / pc_da.var(dim=\"time\")\n",
    "                regression_maps.append(reg_map)\n",
    "        \n",
    "            # Combine into a single DataArray with a 'mode' dimension\n",
    "            regression_map_all = xr.concat(regression_maps, dim=pd.Index(range(max_modes), name='mode'))\n",
    "            #regression_map_all.to_netcdf(output_file)\n",
    "            EOF.to_netcdf(output_file)\n",
    "            \n",
    "        return EOF\n",
    "\n",
    "    def projection(self, varname: str, max_modes):\n",
    "        #need to somehow specify that its historical EOF ONLY - but maybe still calculate for the other ones - need all anomalies???\n",
    "\n",
    "        output_file = self.all_experiments.output_dir / f\"regression_patterns/NAO/psl_mon_historical_HadGEM3-GC31-LL_DJF_NAO_EOF_pattern_1850-2015.nc\"\n",
    "\n",
    "        if output_file.exists():\n",
    "            print(\"loading EOF file for model:\", self.name)\n",
    "            return xr.open_dataset(output_file)\n",
    "\n",
    "        trend = self.calc_linear_trend_per_model['slope']\n",
    "        EOF = self.calc_EOF_concat['eofs']\n",
    "\n",
    "        #weighting the trend ONLY (EOF already weighted)\n",
    "        w = np.sqrt(np.cos(np.radians(trend['lat'])))\n",
    "        w2d, _ = xr.broadcast(w, trend)\n",
    "\n",
    "        #stacking the trend and EOF \n",
    "        trend_w = (trend * w2d).stack(spatial=('lat','lon')).values\n",
    "        EOF = (EOF).stack(spatial=('lat','lon')).values #lat,lon,mode\n",
    "\n",
    "        #transpose for the lstsq\n",
    "        E_matrix = EOF.T\n",
    "\n",
    "        #solve the weighted least squares\n",
    "        c = np.linalg.lstsq(E_matrix, trend_w, rcond=None)[0]\n",
    "\n",
    "        #reconstruct the coefficients\n",
    "        #got to here need to figure out how to extract???\n",
    "        #for i in range(0, max_modes) vibes then extract and just save\n",
    "        #somehow return??? need to think more about the format\n",
    "\n",
    "        return ('this')\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "class ExperimentCalculations:\n",
    "    def __init__(self, folder: Path, all_experiments):\n",
    "        \"\"\"\n",
    "        Initialise an experiment object\n",
    "        Each folder inside this experiment is a model\n",
    "        \"\"\"\n",
    "        self.folder = Path(folder)\n",
    "        \n",
    "        #the experiment name (resturns the last part of the name!!!)\n",
    "        self.name = self.folder.name\n",
    "        self.all_experiments = all_experiments\n",
    "\n",
    "        #this is the dictionary of ModelCalculations objects\n",
    "        self.models = {}\n",
    "\n",
    "        #going through all the sub folders within this experiment folder\n",
    "        #The creating a ModelCalculations object for models folders\n",
    "        for model_folder in self.folder.iterdir():\n",
    "            if model_folder.is_dir():\n",
    "                model_name = model_folder.name\n",
    "\n",
    "                #skipping CNRM-CM6-1 for now\n",
    "                if model_name == 'CNRM-CM6-1':\n",
    "                    continue\n",
    "                    \n",
    "                model = ModelCalculations(model_folder, experiment=self)\n",
    "                self.models[model.name] = model\n",
    "        \n",
    "    def calc_ensemble_mean_all_models(self, varname: str):\n",
    "        \"\"\"\n",
    "        Calculate the ensemble mean for each model within this experiment\n",
    "        Save results to an output dir\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.all_experiments.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        results = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            print(self.name)\n",
    "\n",
    "            ens_mean = model.calc_ensemble_mean(varname)\n",
    "            results[model_name] = ens_mean\n",
    "\n",
    "        return results\n",
    "        \n",
    "    def calc_seasonal_EM_all_models(self, varname: str):\n",
    "        \"\"\"\n",
    "        Calculate the ensemble mean for each model within this experiment\n",
    "        Save results to an output dir\n",
    "        \"\"\"\n",
    "\n",
    "        self.all_experiments.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        results = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            print(self.name)\n",
    "            \n",
    "            seas_ens_mean = model.calc_seasonal_mean_per_model(varname)\n",
    "            results[model_name] = seas_ens_mean\n",
    "\n",
    "        return results\n",
    "\n",
    "    def calc_linear_trend_all_models(self, varname: str):\n",
    "        \"\"\"\n",
    "        Calculate the linear trend from the seasonal ensemble mean\n",
    "        Save results to an output dir\n",
    "        \"\"\"\n",
    "\n",
    "        self.all_experiments.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        results = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            print(self.name)\n",
    "            \n",
    "            trend = model.calc_linear_trend_per_model(varname)\n",
    "            results[model_name] = trend\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def calc_anomalies_all_models(self, varname: str):\n",
    "        \"\"\"\n",
    "        Calculate the anomalies across all ensemble members and models\n",
    "        Will probably add in the steps that happen before - either call here or within calc_anomaly\n",
    "        returns a dict of dict: {model_name: {member_id: anomaly_dataset}}\n",
    "        \"\"\"\n",
    "\n",
    "        self.all_experiments.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "        results = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            model_results = model.calc_anomalies_all_members(varname)\n",
    "            results[model_name] = model_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def calc_EOF_concat_all_models(self, varname: str, max_modes: int):\n",
    "        \"\"\"\n",
    "        To calculate the EOFs across all models\n",
    "        \"\"\"\n",
    "\n",
    "        self.all_experiments.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        results = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            model_results = model.calc_EOF_concat(varname, max_modes)\n",
    "            results[model_name] = model_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def projection_all_models(self, varname: str, max_modes: int):\n",
    "        \"\"\"\n",
    "        Project linear trend onto ALL modes for each model\n",
    "        \"\"\"\n",
    "        self.all_experiments.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        results = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            proj = model.projection(varname, max_modes)\n",
    "            results[model_name] = proj\n",
    "\n",
    "        return results\n",
    "        \n",
    "\n",
    "\n",
    "class AllDataComparisons:\n",
    "    def __init__(self, folder: Path, output_dir: Path):\n",
    "        \"\"\"\n",
    "        Initialise a comparisons object\n",
    "        Will be used to comapre between all models and all experiments.\n",
    "        Pass through the folder up to the experiments - will automatically sort for the experiment available\n",
    "        \"\"\"\n",
    "        self.folder = Path(folder)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        #the experiment name (resturns the last part of the name!!!)\n",
    "\n",
    "        #this is the dictionary of ExperimentCalculations objects\n",
    "        self.experiments = {}\n",
    "\n",
    "        #going through all the sub folders within this experiment folder\n",
    "        #The creating a ModelCalculations object for models folders\n",
    "        for experiment_folder in self.folder.iterdir():\n",
    "            if experiment_folder.is_dir():\n",
    "                experiment = ExperimentCalculations(experiment_folder, all_experiments=self)\n",
    "                self.experiments[experiment.name] = experiment\n",
    "        #for the domain bounds\n",
    "        self.lat_bounds = None\n",
    "        self.lon_bounds = None\n",
    "\n",
    "        self.season = None\n",
    "\n",
    "    def summary(self, show_members: bool = False):\n",
    "        \"\"\"\n",
    "        Print a summary of the experiments, models, and optionally ensemble members.\n",
    "        \"\"\"\n",
    "        print(f\"\\n📊 Summary of AllDataComparisons: '{self.folder.name}'\")\n",
    "        print(f\"  Experiments loaded: {len(self.experiments)}\")\n",
    "\n",
    "        for exp_name, exp in self.experiments.items():\n",
    "            print(f\"  └── Experiment: {exp_name} ({len(exp.models)} models)\")\n",
    "\n",
    "            for model_name, model in exp.models.items():\n",
    "                print(f\"      └── Model: {model_name} ({len(model.members)} members)\")\n",
    "\n",
    "                if show_members:\n",
    "                    for member_id, member in model.members.items():\n",
    "                        print(f\"          └── Member: {member_id}\")\n",
    "                        \n",
    "            \n",
    "    def set_time_bounds_all(self, start_year: int, end_year:int):\n",
    "        \"\"\"\n",
    "        Set the time bounds for all the models in this experiment\n",
    "        \"\"\"\n",
    "\n",
    "        for experiment in self.experiments.values():\n",
    "            for model in experiment.models.values():\n",
    "                model.set_time_bounds(start_year, end_year)\n",
    "\n",
    "    def set_domain(self, lat_bounds: tuple, lon_bounds: tuple):\n",
    "        \"\"\"\n",
    "        Defining the coordingates for the domain for cropping\n",
    "        \"\"\"\n",
    "\n",
    "        self.lat_bounds = lat_bounds\n",
    "        self.lon_bounds = lon_bounds\n",
    "        print(self.lat_bounds, self.lon_bounds)\n",
    "\n",
    "    def set_season(self, season: list[int]):\n",
    "        \"\"\"\n",
    "        Sets the season - list like 12,1,2 is DJF\n",
    "        \"\"\"\n",
    "\n",
    "        self.season = season\n",
    "        print(season)\n",
    "\n",
    "    def crop_to_domain_all_exp(self, da: xr.DataArray):\n",
    "        \"\"\"\n",
    "        Crops to the lat and lon of the domain\n",
    "        lat_bounds = (lat_min, lat_max)\n",
    "        lon_bounds = (lon_min, lon_max)\n",
    "        Assuming data of the format -180 to 180, -90 to 90???\n",
    "        Will add another function to check and if not flag error\n",
    "        Will be a bit for if model is era5 shift the coords at somepoint in the processing data stages.\n",
    "        Need to already have selected the variable\"\"\"\n",
    "\n",
    "        if self and self.lat_bounds and self.lon_bounds:\n",
    "            da = da.sel(\n",
    "                lat=slice(self.lat_bounds[0], self.lat_bounds[1]),\n",
    "                lon = slice(self.lon_bounds[0], self.lon_bounds[1])\n",
    "            )\n",
    "        return da\n",
    "\n",
    "    def add_sum_experiment(self, varname:str):\n",
    "        \"\"\"\n",
    "        Creating a 'sum' object-like thing to be added to the other experiments.\n",
    "        New 'sum' in self.experiments\"\"\"\n",
    "\n",
    "        sum_models = {}\n",
    "\n",
    "        #getting all models names\n",
    "        for model_name in self.get_all_model_names():\n",
    "            model_trends = []\n",
    "\n",
    "            for exp_name, exp in self.experiments.items():\n",
    "                if exp_name == 'historical':\n",
    "                    continue\n",
    "\n",
    "                if model_name in exp.models:\n",
    "                    print(model_name)\n",
    "                    ds = exp.models[model_name].calc_linear_trend_per_model(varname)\n",
    "                    model_trends.append(ds['slope'])\n",
    "                    if 'slope' not in ds:\n",
    "                        print(f\"{model_name} in {exp_name} has no slope\")\n",
    "                        continue                    \n",
    "\n",
    "            if not model_trends:\n",
    "                print(f\"No trends found for {model_name}\")\n",
    "                continue\n",
    "\n",
    "            # Align grids and sum slopes across experiments\n",
    "            aligned = xr.align(*model_trends, join=\"outer\")\n",
    "            slope_sum = sum(a.fillna(0) for a in aligned)\n",
    "            ds_sum = xr.Dataset({\"slope\": slope_sum})\n",
    "            print(f\"Slope sum for {model_name} computed\")\n",
    "\n",
    "            # Fake model object with required method\n",
    "            sum_models[model_name] = SimpleNamespace(\n",
    "                name=model_name,\n",
    "                calc_linear_trend_per_model=lambda v, ds=ds_sum: ds\n",
    "            )\n",
    "\n",
    "        # Add new synthetic experiment\n",
    "        self.experiments[\"sum\"] = SimpleNamespace(\n",
    "            name=\"sum\",\n",
    "            models=sum_models\n",
    "        )\n",
    "\n",
    "        \n",
    "    def project_trend_EOF(self, varname: str, max_modes: int):\n",
    "        \"\"\"\n",
    "        Projection of the linear trend onto the number of EOFs specified\n",
    "        Will weight the trend to match the EOFs subspace (EOFs already weighted)\n",
    "        If it already exists then can just upload the files\n",
    "        Outputs:\n",
    "            - Weighted linear trend\n",
    "            - Projections for number of EOF modes specified\n",
    "            - Residual (weighted linear trend - sum(projections))\n",
    "\n",
    "        Returns the nested dict:\n",
    "            experiment -> model -> xr.Dataset\n",
    "            with variables: projection(projection_mode1, projection_mode2, ..., projection_modeN), residual, weighted_trend\n",
    "        \"\"\"\n",
    "        delta = 165\n",
    "        \n",
    "        #this is basically selecting all models' EOFs for the historical and raising an error if not.\n",
    "        if 'historical' in self.experiments:\n",
    "            exp = self.experiments['historical']\n",
    "            EOFs = exp.calc_EOF_concat_all_models(varname, max_modes)\n",
    "        else:\n",
    "            raise ValueError(f\"historical experiment has not been found\")\n",
    "\n",
    "        all_results = {}\n",
    "        \n",
    "        for experiment in self.experiments.values():\n",
    "            all_results[experiment.name] = {}\n",
    "            #Now looping through all the experiments\n",
    "            trend_dict_hpa = {\n",
    "                model_name: trend_ds * delta / 100\n",
    "                for model_name, trend_ds in experiment.calc_linear_trend_all_models(varname).items()\n",
    "            }\n",
    "            trend_dict = experiment.calc_linear_trend_all_models(varname)\n",
    "            for model_name, trend_ds in trend_dict.items():\n",
    "                #select the correct models EOFs and Trends and then cropping the trend (calculated for entire globe)\n",
    "                eofs = EOFs[model_name]\n",
    "                trend_var = trend_ds['slope']\n",
    "                trend = self.crop_to_domain_all_exp(trend_var)\n",
    "\n",
    "                #apply weights ONLY to the trend\n",
    "                w = np.sqrt(np.cos(np.radians(trend['lat'])))\n",
    "                w2d, _ = xr.broadcast(w, trend)\n",
    "                trend_w2d = trend * w2d\n",
    "    \n",
    "                #stack the spatial dimensions (to solve the least squares)\n",
    "                T = trend_w2d.stack(spatial=('lat', 'lon'))\n",
    "                E = eofs.stack(spatial=('lat', 'lon')).transpose('spatial', 'mode')\n",
    "    \n",
    "                #now solve the least squares to get the coefficients, T (n_spatial,) E (n_spat, n_modes)\n",
    "                T_vals = T.values\n",
    "                E_vals = E.values\n",
    "                c_vals, _, _, _ = np.linalg.lstsq(E_vals, T_vals, rcond=None)\n",
    "                c = xr.DataArray(c_vals, dims=['mode'], coords={'mode': E.mode})\n",
    "\n",
    "\n",
    "                #setting up so that there is a dict for projections\n",
    "                projections = (c*eofs).transpose('mode', 'lat', 'lon')\n",
    "                projections.name = \"projection\"\n",
    "\n",
    "                #the total reconstrcution\n",
    "                reconstructed = projections.sum(dim='mode')\n",
    "                \n",
    "                #finding the residual and adding it and the weighted trend to the nested dict\n",
    "                residual = trend_w2d - reconstructed\n",
    "                residual.name = \"residual\"\n",
    "\n",
    "                #create the dataset\n",
    "                proj_ds = xr.Dataset({\n",
    "                    \"projections\": projections,\n",
    "                    \"residual\": residual,\n",
    "                    \"weighted_trend\": trend_w2d\n",
    "                })\n",
    "\n",
    "                #storing all the projections\n",
    "                all_results[experiment.name][model_name] = proj_ds\n",
    "                \n",
    "                \n",
    "        return all_results\n",
    "\n",
    "    def calc_R2(self, varname: str, max_modes: int):\n",
    "\n",
    "        all_results = self.project_trend_EOF(varname, max_modes)\n",
    "        R2_vals = {}\n",
    "\n",
    "        #remember this is like looping through objects so need to extract name via .name\n",
    "        for experiment in self.experiments.values():\n",
    "            R2_vals[experiment.name] = {}\n",
    "            \n",
    "            for model in experiment.models.values():\n",
    "                ds = all_results[experiment.name][model.name]\n",
    "                \n",
    "                trend_w = ds['weighted_trend']\n",
    "                residual = ds['residual']\n",
    "                projections = ds['projections']\n",
    "\n",
    "                total_var = (trend_w**2).sum().item()\n",
    "                \n",
    "                #creating R2 datastructure\n",
    "                R2_vals[experiment.name][model.name] = {\n",
    "                    \"total\": total_var,\n",
    "                    \"residual\": float(( (residual**2).sum() / total_var).item()),\n",
    "                    \"projections\": {}\n",
    "                }\n",
    "\n",
    "                for mode in projections.mode.values:\n",
    "                    proj = projections.sel(mode=mode)\n",
    "                    R2 = ( (proj**2).sum() / total_var ).item()\n",
    "                    R2_vals[experiment.name][model.name][\"projections\"][int(mode)] = float(R2)\n",
    "\n",
    "        return R2_vals\n",
    "\n",
    "    def R2_plot(self, varname: str, max_modes: int):\n",
    "        \"\"\"\n",
    "        Creates the R2 plot across all the experiments and models\n",
    "        \"\"\"\n",
    "        R2 = self.calc_R2(varname, max_modes)\n",
    "        print(R2)\n",
    "        \n",
    "        experiments = list(R2.keys())\n",
    "    \n",
    "        # Collect all unique models\n",
    "        models = set()\n",
    "        for exp_name in experiments:\n",
    "            models.update(R2[exp_name].keys())\n",
    "        models = sorted(models)\n",
    "        \n",
    "        # Marker styles for models\n",
    "        marker_styles = ['o', 's', '^', 'D', 'v', 'P', '*', 'X', 'h', '1']\n",
    "        model_markers = {model: marker_styles[i % len(marker_styles)] for i, model in enumerate(models)}\n",
    "        \n",
    "        nrows = max_modes + 1  # weighted trend + projections + residual\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(10, 4*nrows), sharex=True)\n",
    "    \n",
    "        # Row labels\n",
    "        row_labels = [f'Mode {i}' for i in range(max_modes)] + ['Residual']\n",
    "    \n",
    "        for i, row_label in enumerate(row_labels):\n",
    "            ax = axes[i]\n",
    "            for model in models:\n",
    "                y_vals = []\n",
    "                x_vals = []\n",
    "                for j, exp_name in enumerate(experiments):\n",
    "                    if model not in R2[exp_name]:\n",
    "                        continue  # skip missing experiment for this model\n",
    "                    model_dict = R2[exp_name][model]\n",
    "                    if i == nrows-1:\n",
    "                        y_val = model_dict['residual']\n",
    "                    else:\n",
    "                        y_val = model_dict['projections'].get(i, np.nan)\n",
    "                    y_vals.append(y_val)\n",
    "                    x_vals.append(j)\n",
    "                if y_vals:\n",
    "                    ax.scatter(x_vals, y_vals, marker=model_markers[model], s=100, label=model if i==0 else \"\")\n",
    "            ax.set_ylabel(\"R²\", fontsize=14)\n",
    "            ax.set_title(row_label, fontsize=16)\n",
    "    \n",
    "        axes[-1].set_xticks(range(len(experiments)))\n",
    "        axes[-1].set_xticklabels(experiments, rotation=45, ha='right', fontsize=12)\n",
    "        axes[0].legend(loc='upper right', fontsize=12)\n",
    "        fig.suptitle(\"R² for all models and experiments\", fontsize=18)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('R2_all_models_and_exp.png')\n",
    "        plt.show()\n",
    "\n",
    "    def get_experiments_per_model(self, model:str):\n",
    "        \"\"\"\n",
    "        Returns the dictionary of all experiments for a given model\n",
    "        Take the model name in and returns dictionary\n",
    "        \"\"\"\n",
    "        return {exp_name: exp_obj.models[model]\n",
    "            for exp_name, exp_obj in self.experiments.items()\n",
    "                if model in exp_obj.models\n",
    "               }\n",
    "\n",
    "    def get_all_model_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of models (list[str)]) across all experiments\n",
    "        \"\"\"\n",
    "        model_names = ({\n",
    "            model_name\n",
    "            for exp_obj in self.experiments.values()\n",
    "            for model_name in exp_obj.models.keys()\n",
    "        })\n",
    "        return sorted(model_names)\n",
    "\n",
    "    def get_all_exp_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of models (list[str)]) across all experiments\n",
    "        \"\"\"\n",
    "        exp_names = ({\n",
    "            exp_name\n",
    "            for exp_name in self.experiments.keys()\n",
    "        })\n",
    "        return sorted(exp_names)\n",
    "                \n",
    "    def projection_steps_plot(self, varname: str, max_modes: int):\n",
    "        \"\"\"\n",
    "        Create the projection plot basically plots the following for each model and experiment\n",
    "        - weighted trend\n",
    "        - bit explained by each mode\n",
    "        - the residual\n",
    "        currently manually setting the max and min - could be good to have a speerate method doing this?...\n",
    "        \"\"\"\n",
    "\n",
    "        #getting the data from all models and experiments (the maps and the R2 values)\n",
    "        all_results = self.project_trend_EOF(varname, max_modes)\n",
    "        R2 = self.calc_R2(varname, max_modes)\n",
    "        \n",
    "        cmap='seismic'\n",
    "        norm_all = mcolors.TwoSlopeNorm(vmin=-2.25, vcenter=0, vmax=2.25)\n",
    "        levels = np.arange(-2.25,2.5,0.05)\n",
    "        lon_min, lon_max = self.lon_bounds\n",
    "        lat_min, lat_max = self.lat_bounds\n",
    "\n",
    "        #okay so getting the list of all model names across all experiments.\n",
    "        #then finding the names/keys for the experimetns in the model\n",
    "        #then plotting if they exist of do the missing step if not.\n",
    "        \n",
    "        exp_names = self.get_all_exp_names()\n",
    "        \n",
    "        for model_name in self.get_all_model_names():\n",
    "            fig, ax = plt.subplots(\n",
    "                nrows=max_modes + 2,\n",
    "                ncols=len(exp_names),\n",
    "                figsize=(28,15),\n",
    "                subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    "            )\n",
    "            \n",
    "            for j, exp_name in enumerate(self.get_all_exp_names()):\n",
    "                try:\n",
    "                    ds = all_results[exp_name][model_name]\n",
    "                    R2_vals = R2[exp_name][model_name]\n",
    "\n",
    "                    #getting the min and max of the weighted trend.\n",
    "                    wt_min = float(ds['weighted_trend'].min())\n",
    "                    wt_max = float(ds['weighted_trend'].max())\n",
    "                    print(f\"{model_name} - {exp_name} - weighted_trend: min={wt_min:.2f}, max={wt_max:.2f}\")\n",
    "\n",
    "                    #plotting the weighted trend if this experiment exists for the model\n",
    "                    contour = ax[0,j].contourf(ds['lon'], ds['lat'], ds['weighted_trend'],\n",
    "                        cmap=cmap, norm=norm_all,\n",
    "                        levels=levels, transform=ccrs.PlateCarree()\n",
    "                    )\n",
    "                    ax[0,j].set_title(f\"{exp_name}\\n\", fontsize=30)\n",
    "\n",
    "                    #projection rows1...rown\n",
    "                    for i, mode in enumerate(range(max_modes), start=1):\n",
    "                        ds[\"projections\"].sel(mode=mode).plot.contourf(\n",
    "                            ax=ax[i, j],\n",
    "                            cmap=cmap, norm=norm_all,\n",
    "                            levels=levels, transform=ccrs.PlateCarree(),\n",
    "                            add_colorbar=False\n",
    "                        )\n",
    "                        ax[i,j].set_title(\"\")\n",
    "                        ax[i,j].text(0.5, -0.05, f\"r² = {R2_vals['projections'][i-1]:.2f}\", transform=ax[i, j].transAxes, ha='center', va='top', fontsize=28)\n",
    "                    \n",
    "                    #residual (last row)\n",
    "                    ds[\"residual\"].plot.contourf(\n",
    "                        ax=ax[max_modes + 1, j], cmap=cmap, norm=norm_all,\n",
    "                        levels=levels, transform=ccrs.PlateCarree(),\n",
    "                        add_colorbar=False\n",
    "                    )\n",
    "                    ax[3,j].text(0.5, -0.05, f\"r² = {R2_vals['residual']:.2f}\", transform=ax[3, j].transAxes, ha='center', va='top', fontsize=28)\n",
    "                    ax[3,j].set_title(\"\")\n",
    "                        \n",
    "                except KeyError:\n",
    "                #if the experiment is missing\n",
    "                    for i in range(max_modes + 2):\n",
    "                        ax[i,j].set_title(f\"{exp_name}\")\n",
    "                        ax[i,j].text(\n",
    "                            0.5,0.5, \"Missing\",\n",
    "                            ha=\"center\", va=\"center\",\n",
    "                            fontsize=28, color=\"gray\", transform=ax[i,j].transAxes\n",
    "                        )\n",
    "                        ax[i,j].set_xticks([])\n",
    "                        ax[i,j].set_yticks([])\n",
    "                        ax[i,j].coastlines\n",
    "                # Add coastlines and formatting\n",
    "                for i in range(4):\n",
    "                    ax[i, j].set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "                    ax[i, j].add_feature(cfeature.COASTLINE, linewidth=1)\n",
    "                    #ax[i, j].set_xlabel('lon', fontsize=28)\n",
    "                    #ax[i, j].set_ylabel('lat', fontsize=28)\n",
    "                    ax[i, j].set_aspect('auto')\n",
    "                    if i == 0:\n",
    "                        ax[0,j].set_title(f\"{exp_name}\\n\", fontsize=30)\n",
    "                    else:\n",
    "                        ax[i,j].set_title(\"\")\n",
    "                    \n",
    "                # Add colorbar\n",
    "            ticks = [-2,-1,0,1,2]\n",
    "\n",
    "            cax = fig.add_axes([0.92, 0.08, 0.015, 0.8])\n",
    "            cbar = fig.colorbar(contour, cax=cax, orientation='vertical', ticks=ticks)\n",
    "            cbar.set_label('MSLP (hPa)', fontsize=32)\n",
    "            cbar.ax.tick_params(labelsize=30)\n",
    "\n",
    "            # add y-axis labels only on the leftmost column\n",
    "            #setting up the row labels\n",
    "            row_labels = [\"Weighted trend\"] + [f\"Mode {m}\" for m in range(max_modes)] + [\"Residual\"]\n",
    "            # Add row labels along the left-hand side using fig.text\n",
    "            for i, label in enumerate(row_labels):\n",
    "                # y coordinate is relative to the whole figure\n",
    "                # compute vertical position from row index\n",
    "                y = (ax[i, 0].get_position().y0 + ax[i, 0].get_position().y1) / 2\n",
    "                fig.text(\n",
    "                    0.1, y, label,\n",
    "                    va=\"center\", ha=\"center\", rotation=90, fontsize=30\n",
    "                )\n",
    "        \n",
    "            fig.suptitle(f\"Projection steps for {model_name}\", x=0.45, fontsize=32)\n",
    "            plt.tight_layout\n",
    "            for file_type in ['png','svg']:\n",
    "                plt.savefig(f\"Figures/Projection_plots_per_model/Projection_{model_name}.{file_type}\", bbox_inches='tight')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "010afa04-417c-46b6-a09e-130cd6420d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 80) (-90, 40)\n",
      "[12, 1, 2]\n",
      "{'hist-GHG': <__main__.ModelCalculations object at 0x7f91c39b4dd0>, 'hist-aer': <__main__.ModelCalculations object at 0x7f91c2a45410>, 'hist-sol': <__main__.ModelCalculations object at 0x7f91cbe181d0>, 'hist-totalO3': <__main__.ModelCalculations object at 0x7f91c2b04c10>, 'hist-volc': <__main__.ModelCalculations object at 0x7f91c3904590>, 'historical': <__main__.ModelCalculations object at 0x7f91cc45b5d0>}\n",
      "ACCESS-ESM1-5\n",
      "loading the linear trend for model: ACCESS-ESM1-5\n",
      "ACCESS-ESM1-5\n",
      "loading the linear trend for model: ACCESS-ESM1-5\n",
      "ACCESS-ESM1-5\n",
      "loading the linear trend for model: ACCESS-ESM1-5\n",
      "ACCESS-ESM1-5\n",
      "loading the linear trend for model: ACCESS-ESM1-5\n",
      "Slope sum for ACCESS-ESM1-5 computed\n",
      "CMCC-CM2-SR5\n",
      "loading the linear trend for model: CMCC-CM2-SR5\n",
      "CMCC-CM2-SR5\n",
      "loading the linear trend for model: CMCC-CM2-SR5\n",
      "CMCC-CM2-SR5\n",
      "loading the linear trend for model: CMCC-CM2-SR5\n",
      "Slope sum for CMCC-CM2-SR5 computed\n",
      "CanESM5\n",
      "loading the linear trend for model: CanESM5\n",
      "CanESM5\n",
      "loading the linear trend for model: CanESM5\n",
      "CanESM5\n",
      "loading the linear trend for model: CanESM5\n",
      "CanESM5\n",
      "loading the linear trend for model: CanESM5\n",
      "CanESM5\n",
      "loading the linear trend for model: CanESM5\n",
      "Slope sum for CanESM5 computed\n",
      "FGOALS-g3\n",
      "loading the linear trend for model: FGOALS-g3\n",
      "FGOALS-g3\n",
      "loading the linear trend for model: FGOALS-g3\n",
      "Slope sum for FGOALS-g3 computed\n",
      "GISS-E2-1-G\n",
      "loading the linear trend for model: GISS-E2-1-G\n",
      "GISS-E2-1-G\n",
      "loading the linear trend for model: GISS-E2-1-G\n",
      "GISS-E2-1-G\n",
      "loading the linear trend for model: GISS-E2-1-G\n",
      "GISS-E2-1-G\n",
      "loading the linear trend for model: GISS-E2-1-G\n",
      "GISS-E2-1-G\n",
      "loading the linear trend for model: GISS-E2-1-G\n",
      "Slope sum for GISS-E2-1-G computed\n",
      "HadGEM3-GC31-LL\n",
      "loading the linear trend for model: HadGEM3-GC31-LL\n",
      "HadGEM3-GC31-LL\n",
      "loading the linear trend for model: HadGEM3-GC31-LL\n",
      "HadGEM3-GC31-LL\n",
      "loading the linear trend for model: HadGEM3-GC31-LL\n",
      "HadGEM3-GC31-LL\n",
      "loading the linear trend for model: HadGEM3-GC31-LL\n",
      "HadGEM3-GC31-LL\n",
      "loading the linear trend for model: HadGEM3-GC31-LL\n",
      "Slope sum for HadGEM3-GC31-LL computed\n",
      "IPSL-CM6A-LR\n",
      "loading the linear trend for model: IPSL-CM6A-LR\n",
      "IPSL-CM6A-LR\n",
      "loading the linear trend for model: IPSL-CM6A-LR\n",
      "Slope sum for IPSL-CM6A-LR computed\n",
      "MIROC6\n",
      "loading the linear trend for model: MIROC6\n",
      "MIROC6\n",
      "loading the linear trend for model: MIROC6\n",
      "MIROC6\n",
      "loading the linear trend for model: MIROC6\n",
      "MIROC6\n",
      "loading the linear trend for model: MIROC6\n",
      "MIROC6\n",
      "loading the linear trend for model: MIROC6\n",
      "Slope sum for MIROC6 computed\n",
      "MPI-ESM1-2-LR\n",
      "loading the linear trend for model: MPI-ESM1-2-LR\n",
      "MPI-ESM1-2-LR\n",
      "loading the linear trend for model: MPI-ESM1-2-LR\n",
      "MPI-ESM1-2-LR\n",
      "loading the linear trend for model: MPI-ESM1-2-LR\n",
      "MPI-ESM1-2-LR\n",
      "loading the linear trend for model: MPI-ESM1-2-LR\n",
      "MPI-ESM1-2-LR\n",
      "loading the linear trend for model: MPI-ESM1-2-LR\n",
      "Slope sum for MPI-ESM1-2-LR computed\n",
      "NorESM2-LM\n",
      "loading the linear trend for model: NorESM2-LM\n",
      "NorESM2-LM\n",
      "loading the linear trend for model: NorESM2-LM\n",
      "NorESM2-LM\n",
      "loading the linear trend for model: NorESM2-LM\n",
      "NorESM2-LM\n",
      "loading the linear trend for model: NorESM2-LM\n",
      "NorESM2-LM\n",
      "loading the linear trend for model: NorESM2-LM\n",
      "Slope sum for NorESM2-LM computed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m exp_name_CanESM5 \u001b[38;5;241m=\u001b[39m all_data\u001b[38;5;241m.\u001b[39mget_experiments_per_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCanESM5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(exp_name_CanESM5)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mall_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection_steps_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpsl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#all_data.add_sum_experiment('psl')\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#print(all_data.experiments['sum'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#                    max_modes=2\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 912\u001b[0m, in \u001b[0;36mAllDataComparisons.projection_steps_plot\u001b[0;34m(self, varname, max_modes)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03mCreate the projection plot basically plots the following for each model and experiment\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;124;03m- weighted trend\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;124;03mcurrently manually setting the max and min - could be good to have a speerate method doing this?...\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m#getting the data from all models and experiments (the maps and the R2 values)\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_trend_EOF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_modes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m R2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_R2(varname, max_modes)\n\u001b[1;32m    915\u001b[0m cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseismic\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[34], line 737\u001b[0m, in \u001b[0;36mAllDataComparisons.project_trend_EOF\u001b[0;34m(self, varname, max_modes)\u001b[0m\n\u001b[1;32m    733\u001b[0m all_results[experiment\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m#Now looping through all the experiments\u001b[39;00m\n\u001b[1;32m    735\u001b[0m trend_dict_hpa \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    736\u001b[0m     model_name: trend_ds \u001b[38;5;241m*\u001b[39m delta \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_name, trend_ds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_sum_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()\n\u001b[1;32m    738\u001b[0m }\n\u001b[1;32m    739\u001b[0m trend_dict \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mcalc_linear_trend_all_models(varname)\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, trend_ds \u001b[38;5;129;01min\u001b[39;00m trend_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m#select the correct models EOFs and Trends and then cropping the trend (calculated for entire globe)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "all_data = AllDataComparisons(Path(\"/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/\"),\n",
    "                              Path(\"/gws/nopw/j04/extant/users/slbennie/\")\n",
    ")\n",
    "\n",
    "#all_data.summary()\n",
    "all_data.set_time_bounds_all(1850, 2014)\n",
    "all_data.set_domain(lat_bounds=(20,80), lon_bounds=(-90,40))\n",
    "all_data.set_season([12,1,2])\n",
    "\n",
    "#all_results = all_data.calc_R2('psl', 2)\n",
    "#R2 = all_results['historical']['HadGEM3-GC31-LL']\n",
    "#all_data.project_trend_EOF(\"psl\",2)\n",
    "#all_data.calc_R2(\"psl\",2)\n",
    "#all_data.projection_steps_plot(\"psl\",2)\n",
    "\n",
    "#print(R2)\n",
    "exp_name_CanESM5 = all_data.get_experiments_per_model('CanESM5')\n",
    "\n",
    "print(exp_name_CanESM5)\n",
    "\n",
    "all_data.projection_steps_plot('psl',2)\n",
    "#all_data.add_sum_experiment('psl')\n",
    "\n",
    "#print(all_data.experiments['sum'])\n",
    "\n",
    "#all_data.projection_steps_plot('psl', 2)\n",
    "#all_data.R2_plot('psl',2)\n",
    "#historical = all_data.experiments['historical']\n",
    "#EOF = historical.calc_EOF_concat_all_models(\n",
    "#                    varname='psl',\n",
    "#                    max_modes=2\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3546341f-afcc-433a-beed-91f791f25e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ExperimentCalculations' object has no attribute 'set_time_bounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m historical \u001b[38;5;241m=\u001b[39m ExperimentCalculations(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/\u001b[39m\u001b[38;5;124m\"\u001b[39m), Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gws/nopw/j04/extant/users/slbennie/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mhistorical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_time_bounds\u001b[49m(\u001b[38;5;241m1850\u001b[39m, \u001b[38;5;241m2014\u001b[39m)\n\u001b[1;32m      3\u001b[0m historical\u001b[38;5;241m.\u001b[39mset_domain(lat_bounds\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m80\u001b[39m), lon_bounds\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m90\u001b[39m,\u001b[38;5;241m40\u001b[39m))\n\u001b[1;32m      4\u001b[0m historical\u001b[38;5;241m.\u001b[39mset_season([\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ExperimentCalculations' object has no attribute 'set_time_bounds'"
     ]
    }
   ],
   "source": [
    "historical = ExperimentCalculations(Path(\"/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/\"), Path(\"/gws/nopw/j04/extant/users/slbennie/\"))\n",
    "historical.set_time_bounds(1850, 2014)\n",
    "historical.set_domain(lat_bounds=(20,80), lon_bounds=(-90,40))\n",
    "historical.set_season([12,1,2])\n",
    "\n",
    "linear_trend = historical.projection_all_models(\n",
    "    varname='psl',\n",
    "    max_modes=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#historical_ens_mean = historical.calc_ensemble_mean_all_models('psl', Path(\"/gws/nopw/j04/extant/users/slbennie/ens_mean_spat/psl/historical/\"))\n",
    "\n",
    "# Compute anomalies for all ensemble members and models\n",
    "#anomaly_results = historical.calc_anomalies_all_models(\n",
    "#    varname=\"psl\",\n",
    "#    output_dir=Path(\"/gws/nopw/j04/extant/users/slbennie/psl_anomalies/historical/\")\n",
    "#)\n",
    "\n",
    "#print(anomaly_results['CanESM5']['r10i1p1f1'].values)\n",
    "\n",
    "#compute the EOF for all models\n",
    "\n",
    "#HadGEM3 = historical.models['HadGEM3-GC31-LL']\n",
    "#EOF = historical.calc_EOF_concat_all_models(\n",
    "#    output_dir = Path('/gws/nopw/j04/extant/users/slbennie/EOF/'),\n",
    "#                    varname='psl',\n",
    "#                    max_modes=2\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6ffd987-e575-41c1-ab2d-94fb69962be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'psl' (year: 165, lat: 25, lon: 53)> Size: 2MB\n",
      "[218625 values with dtype=float64]\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 200B 20.0 22.5 25.0 27.5 30.0 ... 72.5 75.0 77.5 80.0\n",
      "  * lon      (lon) float64 424B -90.0 -87.5 -85.0 -82.5 ... 32.5 35.0 37.5 40.0\n",
      "  * year     (year) int64 1kB 1850 1851 1852 1853 1854 ... 2011 2012 2013 2014\n",
      "    season   <U3 12B ...\n"
     ]
    }
   ],
   "source": [
    "print(anomaly_results['CanESM5']['r10i1p1f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f65a5ea-dffa-4c8d-99cf-99a5c2cff3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cftime.Datetime360Day(1850, 1, 16, 0, 0, 0, 0, has_year_zero=True), cftime.Datetime360Day(2014, 12, 16, 0, 0, 0, 0, has_year_zero=True))\n",
      "(cftime.Datetime360Day(1850, 1, 16, 0, 0, 0, 0, has_year_zero=True), cftime.Datetime360Day(2014, 12, 16, 0, 0, 0, 0, has_year_zero=True))\n",
      "360_day\n",
      "<xarray.DataArray 'psl' (time: 1980, lat: 71, lon: 144)> Size: 162MB\n",
      "[20243520 values with dtype=float64]\n",
      "Coordinates:\n",
      "  * time     (time) object 16kB 1850-01-16 00:00:00 ... 2014-12-16 00:00:00\n",
      "  * lat      (lat) float64 568B -87.5 -85.0 -82.5 -80.0 ... 80.0 82.5 85.0 87.5\n",
      "  * lon      (lon) float64 1kB -180.0 -177.5 -175.0 -172.5 ... 172.5 175.0 177.5\n",
      "Attributes:\n",
      "    standard_name:  air_pressure_at_mean_sea_level\n",
      "    long_name:      Sea Level Pressure\n",
      "    comment:        Sea Level Pressure\n",
      "    units:          Pa\n",
      "    original_name:  mo: (stash: m01s16i222, lbproc: 128)\n",
      "    cell_methods:   area: time: mean\n",
      "    cell_measures:  area: areacella\n",
      "Loading the exisiting file for the spatial ensemble mean from: /gws/nopw/j04/extant/users/slbennie/ens_mean_spat/psl/historical/HadGEM3-GC31-LL/psl_mon_historical_HadGEM3-GC31-LL_spatial_DJF_EM_1850-2015.nc\n"
     ]
    }
   ],
   "source": [
    "model = ModelCalculations(Path(\"/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/HadGEM3-GC31-LL/\"))\n",
    "\n",
    "model.set_time_bounds(1850, 2014)\n",
    "\n",
    "member1 = next(iter(model.members.values()))\n",
    "\n",
    "#so model.members is my dictionary of ensemble members, keys are member_ids values are EnsembleMemberCalculations objects\n",
    "#\"r1i1p1f1\": <EnsembleMemberCalculations object>, ...\n",
    "#the .values() returns all the values\n",
    "#iter() lets you iterate throught the objects one by one\n",
    "#next() grabs the firt item.\n",
    "\n",
    "#EQUIVALENT\n",
    "#first_key = list(model.members.keys())[0]\n",
    "#member1 = model.members[first_key]\n",
    "\n",
    "print(model.time_bounds)\n",
    "print(member1.get_calendar_type())  # e.g., 'noleap'\n",
    "print(member1.get_time_bounds())    # (start, end) as cftime objects\n",
    "\n",
    "print(member1.select_data('psl'))\n",
    "\n",
    "ens_mean = model.calc_ensemble_mean(Path(\"/gws/nopw/j04/extant/users/slbennie/ens_mean_spat/psl/historical/HadGEM3-GC31-LL/psl_mon_historical_HadGEM3-GC31-LL_spatial_DJF_EM_1850-2015.nc\"), 'psl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d18698c3-492d-4469-9d93-35b7fe46a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r11i1p1f3', 'r12i1p1f3', 'r13i1p1f3', 'r14i1p1f3', 'r15i1p1f3', 'r16i1p1f3', 'r17i1p1f3', 'r18i1p1f3', 'r19i1p1f3', 'r1i1p1f3', 'r20i1p1f3', 'r21i1p1f3', 'r22i1p1f3', 'r23i1p1f3', 'r24i1p1f3', 'r25i1p1f3', 'r26i1p1f3', 'r27i1p1f3', 'r28i1p1f3', 'r29i1p1f3', 'r2i1p1f3', 'r30i1p1f3', 'r31i1p1f3', 'r32i1p1f3', 'r33i1p1f3', 'r34i1p1f3', 'r35i1p1f3', 'r36i1p1f3', 'r37i1p1f3', 'r38i1p1f3', 'r39i1p1f3', 'r3i1p1f3', 'r40i1p1f3', 'r41i1p1f3', 'r42i1p1f3', 'r43i1p1f3', 'r44i1p1f3', 'r45i1p1f3', 'r46i1p1f3', 'r47i1p1f3', 'r48i1p1f3', 'r49i1p1f3', 'r4i1p1f3', 'r50i1p1f3', 'r51i1p1f3', 'r52i1p1f3', 'r53i1p1f3', 'r54i1p1f3', 'r55i1p1f3', 'r56i1p1f3', 'r57i1p1f3', 'r58i1p1f3', 'r59i1p1f3', 'r5i1p1f3', 'r60i1p1f3']\n",
      "1850\n"
     ]
    }
   ],
   "source": [
    "#idea is to create objects for all the ensembles within the folder\n",
    "#within the class there will be a method to extract the member_id from the filename\n",
    "\n",
    "\n",
    "folder = Path('/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/HadGEM3-GC31-LL/')\n",
    "\n",
    "#collecting all the ensemble members in this folder - slightly tricky in that depends on this folder not really being altered?\n",
    "ensemble_members = [\n",
    "    EnsembleMemberCalculations(file)\n",
    "    for file in folder.glob(\"*.nc\")\n",
    "]\n",
    "\n",
    "print([member.member_id for member in ensemble_members])\n",
    "\n",
    "#loading in the data\n",
    "ds = ensemble_members[0].load_data()\n",
    "ensemble_members[0].get_time_bounds('1850', '2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419a3683-10fe-4331-bdfc-d9c7845ed696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<xarray.Dataset> Size: 169MB\n",
      "Dimensions:    (time: 1980, bnds: 2, lat: 71, lon: 144)\n",
      "Coordinates:\n",
      "  * time       (time) object 16kB 1850-01-16 00:00:00 ... 2014-12-16 00:00:00\n",
      "  * lat        (lat) float64 568B -87.5 -85.0 -82.5 -80.0 ... 82.5 85.0 87.5\n",
      "  * lon        (lon) float64 1kB -180.0 -177.5 -175.0 ... 172.5 175.0 177.5\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    time_bnds  (time, bnds) object 32kB ...\n",
      "    lat_bnds   (time, lat, bnds) float64 2MB ...\n",
      "    lon_bnds   (time, lon, bnds) float64 5MB ...\n",
      "    psl        (time, lat, lon) float64 162MB ...\n",
      "Attributes: (12/46)\n",
      "    Conventions:            CF-1.7 CMIP-6.2\n",
      "    activity_id:            CMIP\n",
      "    branch_method:          standard\n",
      "    branch_time_in_child:   0.0\n",
      "    branch_time_in_parent:  0.0\n",
      "    creation_date:          2019-06-19T12:06:35Z\n",
      "    ...                     ...\n",
      "    title:                  HadGEM3-GC31-LL output prepared for CMIP6\n",
      "    variable_id:            psl\n",
      "    variant_label:          r1i1p1f3\n",
      "    license:                CMIP6 model data produced by the Met Office Hadle...\n",
      "    cmor_version:           3.4.0\n",
      "    tracking_id:            hdl:21.14100/0c92cf9a-1f9c-41b3-9192-fb608624dc98\n"
     ]
    }
   ],
   "source": [
    "member1 = EnsembleMemberCalculations(\n",
    "    member_id='1',\n",
    "    data_path = Path('/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/HadGEM3-GC31-LL/psl_mon_historical_HadGEM3-GC31-LL_r1i1p1f3_interp.nc')\n",
    ")\n",
    "\n",
    "ds = member1.load_data()\n",
    "\n",
    "print(member1.member_id)\n",
    "print(ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
