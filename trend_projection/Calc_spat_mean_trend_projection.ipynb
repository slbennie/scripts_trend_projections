{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1addd5f-6e2a-43fa-82f8-b155037c3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cftime\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import linregress\n",
    "from eofs.xarray import Eof\n",
    "from eofs.examples import example_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe5a59a-c91a-4956-9ee3-e202c8f4e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the correct calendar (model dependent.)\n",
    "def get_time_bounds(calendar_type, start, end):\n",
    "    #1850-2015 all of 2014 - none of 2015.\n",
    "    if calendar_type == cftime.DatetimeNoLeap:\n",
    "        return cftime.DatetimeNoLeap(start,1,16), cftime.DatetimeNoLeap(end,1,16)\n",
    "    elif calendar_type == cftime.Datetime360Day:\n",
    "        return cftime.Datetime360Day(start,1,16), cftime.Datetime360Day(end-1,12,16)\n",
    "    else:\n",
    "        return datetime(start,1,16), datetime(end,1,16)\n",
    "\n",
    "#finding all the models that have ensembles for that experiment.\n",
    "def get_models_for_experiment(experiment):\n",
    "    if experiment == 'historical':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','FGOALS-g3','GISS-E2-1-G','HadGEM3-GC31-LL','IPSL-CM6A-LR','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-aer':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','CNRM-CM6-1','FGOALS-g3','GISS-E2-1-G','HadGEM3-GC31-LL','IPSL-CM6A-LR','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-GHG':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','CNRM-CM6-1','FGOALS-g3','GISS-E2-1-G','HadGEM3-GC31-LL','IPSL-CM6A-LR','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-sol':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','GISS-E2-1-G','HadGEM3-GC31-LL','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-totalO3':\n",
    "        model = ['CanESM5','GISS-E2-1-G','HadGEM3-GC31-LL','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "    elif experiment == 'hist-volc':\n",
    "        model = ['ACCESS-ESM1-5','CanESM5','CMCC-CM2-SR5','GISS-E2-1-G','HadGEM3-GC31-LL','MIROC6','MPI-ESM1-2-LR','NorESM2-LM']\n",
    "        \n",
    "    return model\n",
    "\n",
    "#Cropping CVDP data to the North Atlantic sector - requires some shifting of 0 of the lat lon coordinate system.\n",
    "def CVDP_EM_crop_NA_sector(filename, pattern):\n",
    "    #function which will crop the historical ensemble mean CVDP output to the NA sector\n",
    "    ds = xr.open_dataset(filename)\n",
    "    ds = ds[pattern]\n",
    "    \n",
    "    #finding the longitudes that are greater than 180\n",
    "    new_lon = np.where(ds.lon > 179, ds.lon -360, ds.lon)\n",
    "    \n",
    "    #creating a copy of the data array where the longitudes have been shifted\n",
    "    ds_shifted = ds.copy()\n",
    "    ds_shifted.coords['lon'] = new_lon\n",
    "    \n",
    "    #Now need to make sure they are in the correct order and then re-index to make sure the lon get put to match the sorted lon\n",
    "    sorted_lon = np.sort(ds_shifted.lon)\n",
    "    ds_shifted = ds_shifted.sel(lon=sorted_lon)\n",
    "    \n",
    "    historical_NAO_EM_shifted = ds_shifted.sel(lat=slice(20,80), lon=slice(-90,40))\n",
    "\n",
    "    return historical_NAO_EM_shifted\n",
    "\n",
    "#Crops to the North Atlantic sector - for the LESFMIP data NOT processed by CVDP\n",
    "def open_cropNA_unitshPA(filename):\n",
    "    #function to crop an ensemble member to the north atlantic region\n",
    "    data = xr.open_dataset(filename)\n",
    "    data_NA = data.sel(lat=slice(20,80), lon=slice(-90,40))/100\n",
    "\n",
    "    return data_NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc9b16f-c17e-4c4c-be0f-e752d43f386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the seasonal and annual ensemble spatial means.\n",
    "\n",
    "def calculate_spatial_ensemble_mean(file_paths, output_file, variable):\n",
    "    #Will be passing through an experiment's model's ensembles.\n",
    "    #opens all the files given by filepath (basically opens all the ensembles)\n",
    "    ds = xr.open_mfdataset(file_paths, combine='nested', concat_dim='ensemble')\n",
    "\n",
    "    #calculate the mean\n",
    "    mean = ds[variable].mean(dim='ensemble')\n",
    "\n",
    "    #save the ensemble mean to the a .nc file\n",
    "    mean.to_netcdf(output_file)\n",
    "    print('saved')\n",
    "\n",
    "    ds.close()\n",
    "    return mean\n",
    "\n",
    "def calculate_seasonal_spatial_ensemble_mean_djf(file_path, var, seas, output_file, year_init, year_final):\n",
    "    #opening dataset\n",
    "    print('in function')\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    #checking it is a datetime object\n",
    "    ds['time'] = xr.decode_cf(ds).time\n",
    "\n",
    "    calendar = type(ds.time.values[0])\n",
    "    \n",
    "    start,end = get_time_bounds(calendar, year_init, year_final)\n",
    "\n",
    "    #selecting the psl variable within time bounds\n",
    "    variable = ds[var].sel(time=slice(start, end))\n",
    "    \n",
    "    #Filter for the desired season (e.g., DJF)\n",
    "    season_mask = variable.time.dt.season == seas\n",
    "    ds_months_seas = variable.sel(time=season_mask)\n",
    "    \n",
    "    #assign and adjust year (DJF split over two years so increasing the year of december and then grouping and finding the mean)\n",
    "    ds_months_seas = ds_months_seas.assign_coords(year=ds_months_seas['time'].dt.year)\n",
    "    ds_months_seas['year'] = ds_months_seas['year'].where(ds_months_seas['time'].dt.month != 12, ds_months_seas['year'] + 1)\n",
    "    #ds_months_seas = ds_months_seas.set_coords('year')\n",
    "    \n",
    "    # average over DJF months for each year\n",
    "    ds_season = ds_months_seas.groupby('year').mean(dim='time')\n",
    "    ds_season.to_netcdf(output_file)\n",
    "    print('saved file')\n",
    "    return ds_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae7eb3a-3a16-4297-8245-09a2195c96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined for calculating the linear trend\n",
    "def calculate_linear_trend_spat_pattern(file_path, variable, output_file):\n",
    "    # Open dataset and extract variable\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    da = ds[variable]\n",
    "\n",
    "    time = ds['year'].values\n",
    "    lat = ds['lat'].values\n",
    "    lon = ds['lon'].values\n",
    "    time_numeric = np.arange(len(time))\n",
    "\n",
    "    slope = np.full((len(lat), len(lon)), np.nan)\n",
    "    intercept = np.full((len(lat), len(lon)), np.nan)\n",
    "    p_value = np.full((len(lat), len(lon)), np.nan)\n",
    "    stderr = np.full((len(lat), len(lon)), np.nan)\n",
    "\n",
    "    for i in range(len(lat)):\n",
    "        for j in range(len(lon)):\n",
    "            ts = da[:, i, j].values\n",
    "            if np.all(np.isfinite(ts)):\n",
    "                reg = linregress(time_numeric, ts)\n",
    "                slope[i, j] = reg.slope\n",
    "                intercept[i, j] = reg.intercept\n",
    "                p_value[i, j] = reg.pvalue\n",
    "                stderr[i, j] = reg.stderr\n",
    "\n",
    "    from scipy.stats import t\n",
    "    n = len(time_numeric)\n",
    "    df = n - 2\n",
    "    alpha = 0.05\n",
    "    t_crit = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "    ci_lower = slope - t_crit * stderr\n",
    "    ci_upper = slope + t_crit * stderr\n",
    "\n",
    "    slope_da = xr.DataArray(slope, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope\")\n",
    "    intercept_da = xr.DataArray(intercept, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"intercept\")\n",
    "    p_value_da = xr.DataArray(p_value, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"p_value\")\n",
    "    ci_lower_da = xr.DataArray(ci_lower, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope_CI_lower\")\n",
    "    ci_upper_da = xr.DataArray(ci_upper, coords=[lat, lon], dims=[\"lat\", \"lon\"], name=\"slope_CI_upper\")\n",
    "\n",
    "    # Save to one combined netCDF file\n",
    "    combined_ds = xr.Dataset({\n",
    "        \"slope\": slope_da,\n",
    "        \"intercept\": intercept_da,\n",
    "        \"p_value\": p_value_da,\n",
    "        \"slope_CI_lower\": ci_lower_da,\n",
    "        \"slope_CI_upper\": ci_upper_da\n",
    "    })\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8661f5d2-4c85-48be-9828-8494f9fc6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the regression map\n",
    "def calculate_regression_map(anomalies, trend, mode, trend_var, e, m, period):\n",
    "    #this will project a trend onto the regression map.\n",
    "    #psl anomalies are linearly regressed onto the PC timeseries (the amount that the EOF's amplitude changes with time)\n",
    "\n",
    "    #setting up output files paths for the projection and the residual\n",
    "    output_regression_map = '/gws/nopw/j04/extant/users/slbennie/regression_patterns/'+mode+'/psl_mon_'+e+'_'+m+'_DJF_'+mode+'_regression_map_'+period+'.nc'\n",
    "\n",
    "\n",
    "    #selecting the mode of which EOF to calculate\n",
    "    if mode == 'NAO':\n",
    "        mode_number = 0\n",
    "    elif mode == 'EA':\n",
    "        mode_number = 1\n",
    "\n",
    "    #opening up all anomaly files and cropping to NA and converting into hPa (anomaly data is in Pa)\n",
    "    anomaly_list = [open_cropNA_unitshPA(f) for f in anomalies]\n",
    "\n",
    "    #selecting the psl data and concatenating list of data arrays.\n",
    "    all_anomalies = xr.concat(anomaly_list, dim='ensemble')['psl']  # Shape: (ensemble, time, lat, lon)\n",
    "    print(all_anomalies.dims)\n",
    "        \n",
    "    #Flatten ensemble and year into one time dimension (needs to be called time for the pcs function to work later)\n",
    "    all_anomalies_stacked = all_anomalies.stack(time=('ensemble', 'year'))\n",
    "    all_anomalies_stacked = all_anomalies_stacked.reset_index('time', drop=True)\n",
    "    all_anomalies_stacked = all_anomalies_stacked.transpose('time', 'lat', 'lon')\n",
    "\n",
    "    #basically weighting so that each grid cell has influence actually proportional to its area\n",
    "    coslat = np.cos(np.deg2rad(all_anomalies_stacked.coords['lat'].values)).clip(0., 1.)\n",
    "    wgts = np.sqrt(coslat)[..., np.newaxis]\n",
    "\n",
    "    #EOF solver\n",
    "    solver = Eof(all_anomalies_stacked, weights=wgts)\n",
    "    \n",
    "    #finding the pattern of the EOF - unitless\n",
    "    EOF_pattern = solver.eofs(neofs=mode_number+1).sel(mode=mode_number)\n",
    "        \n",
    "    #getting the EA Pattern's PC\n",
    "    #using pcscaling=1 for a normalised PC. If not normalised need to divide by the variance of PC ((pc.std(dim='time'))**2) to find the regression map.\n",
    "    pc = solver.pcs(npcs=mode_number+1, pcscaling=1).sel(mode=mode_number)\n",
    "        \n",
    "    #finding regression_map = pattern of psl anomalies regressed onto EA PC, kinda which bits of the trend link to this pattern, units of hPa/unit of PC\n",
    "    #how psl anomalies change spatially for a one-unit change in the PC\n",
    "    regression_map = (all_anomalies_stacked * pc).mean(dim='time')\n",
    "\n",
    "    #making sure that the patterns match what they should for the NAO and EA patterns (basically fixing for sign conventions to make sure physical)\n",
    "    if mode == 'NAO' and regression_map.sel(lat=50, lon=-30, method='nearest') < 0:\n",
    "        regression_map *= -1\n",
    "        pc *= -1\n",
    "\n",
    "    if mode == 'NAO' and regression_map.sel(lat=50, lon=-25, method='nearest') > 0:\n",
    "        regression_map *= -1\n",
    "        pc *= -1\n",
    "        \n",
    "    #outputting .nc files for plotting\n",
    "\n",
    "    \n",
    "    regression_map.name = 'regression_'+mode+'_djf'\n",
    "    regression_map.to_netcdf(output_regression_map)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f147d4f4-8bd0-47c8-ad9f-eba97e905596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions defined for calculating the projections\n",
    "def project_onto_regression(trend_raw, regression_map, trend_var, mode, e, m, period):\n",
    "    #function which will project a trend (lat,lon) in hPa onto a spatial pattern (lat,lon) hPa to get a single NAO index value\n",
    "    #will then calculate the residual (trend - mode_congruent part) and saves both the NAO congruent part and the residual\n",
    "    #in the same folder, output_file is for the NAO/EA_congruent part filename\n",
    "    #can then change the input spat pattern to calculate the projection onto other eofs, e.g. the EAP\n",
    "\n",
    "\n",
    "    if isinstance(trend_raw, xr.DataArray):\n",
    "        trend = trend_raw\n",
    "    else:\n",
    "        print('here')\n",
    "        trend = trend_raw[trend_var]\n",
    "        \n",
    "    # Weight psl data by coslat to account for grid cell area decreasing with latitude\n",
    "    weights = np.cos(np.radians(trend[\"lat\"].values))\n",
    "    weights_2d = weights[:, np.newaxis]\n",
    "\n",
    "    # weight psl (or another variable) anomalies by area of each gridcell\n",
    "    weighted_trend = trend * weights_2d\n",
    "    weighted_regression = regression_map * weights_2d\n",
    "\n",
    "    # flatten both of the fields so that they are both 1D\n",
    "    trend_flat = weighted_trend.stack(spatial=('lat','lon'))\n",
    "    regression_flat = weighted_regression.stack(spatial=('lat','lon'))\n",
    "\n",
    "    #replace any NaNs with zeros to stop any weird stuff happening\n",
    "    trend_flat = trend_flat.fillna(0)\n",
    "    regression_flat = regression_flat.fillna(0)\n",
    "\n",
    "    #Now do the dot product which is the projection\n",
    "    dot_product = (trend_flat * regression_flat).sum().item()\n",
    "\n",
    "    #calculating the index - or I guess the PC?????\n",
    "    index = dot_product / (regression_flat**2).sum().item()\n",
    "\n",
    "    #Now multiplying the pattern by the index and returning that too\n",
    "    projection = index * regression_map\n",
    "    residual = trend - projection\n",
    "    \n",
    "    projection.name = 'projection_'+mode+'_djf'\n",
    "    residual.name = 'residual_'+mode+'_djf'\n",
    "\n",
    "    output_projection = '/gws/nopw/j04/extant/users/slbennie/projection_indicies/NAtlantic_forced_trends/'+e+'/'+m+'/psl_mon_'+e+'_'+m+'_DJF_'+mode+'_projection_'+period+'.nc'\n",
    "    output_residual = '/gws/nopw/j04/extant/users/slbennie/projection_indicies/NAtlantic_forced_trends/'+e+'/'+m+'/psl_mon_'+e+'_'+m+'_DJF_'+mode+'_residual_'+period+'.nc'\n",
    "    \n",
    "    #outputting .nc files for plotting\n",
    "    projection.to_netcdf(output_projection)\n",
    "    residual.to_netcdf(output_residual)\n",
    "    \n",
    "    return projection, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a243433-6e38-4a71-8428-7076cf92c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the experiments and choose a single mode? could just go ahead and calculate for all of them\n",
    "#trend over the whole period?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b19a671-b5a6-4c36-a85a-5de458e5a9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the spatial ensemble means and trends for experiment e: historical\n",
      "Model:  MPI-ESM1-2-LR\n",
      "Calculating the ensemble mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r17i1p1f1_interp.nc', file descriptor = 67, errno = 103, error message = 'Software caused connection abort', buf = 0x7fc12da33b10, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r16i1p1f1_interp.nc', file descriptor = 66, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc105a2e1b0, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r15i1p1f1_interp.nc', file descriptor = 65, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc135693f50, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r14i1p1f1_interp.nc', file descriptor = 64, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc129aca580, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r13i1p1f1_interp.nc', file descriptor = 63, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc13d7fec60, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r12i1p1f1_interp.nc', file descriptor = 62, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc10d6713e0, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r11i1p1f1_interp.nc', file descriptor = 61, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc139b50660, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r10i1p1f1_interp.nc', file descriptor = 60, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc121acd410, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r9i1p1f1_interp.nc', file descriptor = 110, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc0ec759c30, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r8i1p1f1_interp.nc', file descriptor = 109, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc115b2a4b0, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r7i1p1f1_interp.nc', file descriptor = 108, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc119a58820, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) thread 0:\n",
      "  #000: H5D.c line 1061 in H5Dread(): can't synchronously read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #001: H5D.c line 1008 in H5D__read_api_common(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #002: H5VLcallback.c line 2092 in H5VL_dataset_read_direct(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #003: H5VLcallback.c line 2048 in H5VL__dataset_read(): dataset read failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Read failed\n",
      "  #004: H5VLnative_dataset.c line 373 in H5VL__native_dataset_read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #005: H5Dio.c line 401 in H5D__read(): can't read data\n",
      "    major: Dataset\n",
      "    minor: Read failed\n",
      "  #006: H5Dchunk.c line 2919 in H5D__chunk_read(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #007: H5Dchunk.c line 4534 in H5D__chunk_lock(): unable to read raw data chunk\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #008: H5Fio.c line 99 in H5F_shared_block_read(): read through page buffer failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #009: H5PB.c line 694 in H5PB_read(): read through metadata accumulator failed\n",
      "    major: Page Buffering\n",
      "    minor: Read failed\n",
      "  #010: H5Faccum.c line 247 in H5F__accum_read(): driver read request failed\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n",
      "  #011: H5FDint.c line 259 in H5FD_read(): driver read request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Read failed\n",
      "  #012: H5FDsec2.c line 702 in H5FD__sec2_read(): file read failed: time = Wed Jul  2 10:57:42 2025\n",
      ", filename = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/historical/MPI-ESM1-2-LR/psl_mon_historical_MPI-ESM1-2-LR_r45i1p1f1_interp.nc', file descriptor = 99, errno = 107, error message = 'Transport endpoint is not connected', buf = 0x7fc0f4831240, total read size = 81792, bytes this sub-read = 81792, bytes actually read = 18446744073709551615, offset = 0\n",
      "    major: Low-level I/O\n",
      "    minor: Read failed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NetCDF: HDF error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/api.py:1369\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m writer, store\n\u001b[0;32m-> 1369\u001b[0m writes \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, BytesIO):\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/common.py:267\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[0;34m(self, compute, chunkmanager_store_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     chunkmanager_store_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 267\u001b[0m delayed_store \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchunkmanager_store_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msources \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/namedarray/daskmanager.py:249\u001b[0m, in \u001b[0;36mDaskManager.store\u001b[0;34m(self, sources, targets, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m store\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/array/core.py:1229\u001b[0m, in \u001b[0;36mstore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1228\u001b[0m store_dsk \u001b[38;5;241m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[0;32m-> 1229\u001b[0m \u001b[43mcompute_as_if_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mArray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_dsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/base.py:403\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[0;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m dsk2 \u001b[38;5;241m=\u001b[39m optimization_function(\u001b[38;5;28mcls\u001b[39m)(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/threaded.py:90\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 90\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/local.py:512\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/local.py:320\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/local.py:225\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    224\u001b[0m task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 225\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/core.py:127\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/dask/array/core.py:118\u001b[0m, in \u001b[0;36mgetter\u001b[0;34m(a, b, asarray, lock)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m asarray \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_arraylike(c) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, np\u001b[38;5;241m.\u001b[39mmatrix)):\n\u001b[0;32m--> 118\u001b[0m         c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(c)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/core/indexing.py:574\u001b[0m, in \u001b[0;36mImplicitToExplicitIndexingAdapter.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: np\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/core/indexing.py:577\u001b[0m, in \u001b[0;36mImplicitToExplicitIndexingAdapter.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/core/indexing.py:788\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/core/indexing.py:651\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:100\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/core/indexing.py:1015\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[0;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[1;32m   1014\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[0;32m-> 1015\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:113\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper._getitem\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    112\u001b[0m         original_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 113\u001b[0m         array \u001b[38;5;241m=\u001b[39m getitem(original_array, key)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:4981\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5953\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable._get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2113\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NetCDF: HDF error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#find spatial mean across the ensembles\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculating the ensemble mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mcalculate_spatial_ensemble_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#To use once spatial ensemble mean is calculated\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculating the seasonal \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mseas\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m spatial ensemble mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mcalculate_spatial_ensemble_mean\u001b[0;34m(file_paths, output_file, variable)\u001b[0m\n\u001b[1;32m      9\u001b[0m mean \u001b[38;5;241m=\u001b[39m ds[variable]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#save the ensemble mean to the a .nc file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m ds\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/core/dataarray.py:4139\u001b[0m, in \u001b[0;36mDataArray.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   4135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4136\u001b[0m     \u001b[38;5;66;03m# No problems with the name - so we're fine!\u001b[39;00m\n\u001b[1;32m   4137\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dataset()\n\u001b[0;32m-> 4139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   4140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4147\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4150\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/api.py:1376\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multifile \u001b[38;5;129;01mand\u001b[39;00m compute:\n\u001b[0;32m-> 1376\u001b[0m         \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compute:\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:577\u001b[0m, in \u001b[0;36mNetCDF4DataStore.close\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.11/site-packages/xarray/backends/file_manager.py:234\u001b[0m, in \u001b[0;36mCachingFileManager.close\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    232\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key, default)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2618\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.close\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2585\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset._close\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2113\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NetCDF: HDF error"
     ]
    }
   ],
   "source": [
    "#notes find a way to update period so its nicer.\n",
    "#also could fix folder so that I use like a home dir etc.\n",
    "#anomalies have already been calculated separetly for each model so preusming they are all correct this should run?...\n",
    "home = '/gws/nopw/j04/extant/users/slbennie/'\n",
    "\n",
    "\n",
    "variable = 'psl'\n",
    "period = '1850-2015'\n",
    "experiment = ['historical', 'hist-aer', 'hist-GHG', 'hist-sol','hist-totalO3','hist-volc']\n",
    "model = ['MPI-ESM1-2-LR']\n",
    "modes = ['NAO', 'EA']\n",
    "seas = 'DJF'\n",
    "\n",
    "for e in experiment:\n",
    "    print('Calculating the spatial ensemble means and trends for experiment e:', e)\n",
    "    for m in model:\n",
    "        print('Model: ',m)\n",
    "        #getting the LESFMIP file paths\n",
    "        folder_path = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/'+e+'/'+m+'/'\n",
    "        file_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if e in filename and m in filename]\n",
    "\n",
    "        #creating output files for the mean across ensembles and the djf mean\n",
    "        output_file = home+'ens_mean_spat/psl/'+e+'/'+m+'/'+variable+'_mon_'+e+'_'+m+'_spatial_EM.nc'\n",
    "        output_file_djf = output_file.replace('spatial_EM', 'spatial_'+seas+'_EM_'+period)\n",
    "        \n",
    "        #find spatial mean across the ensembles\n",
    "        print('Calculating the ensemble mean')\n",
    "        calculate_spatial_ensemble_mean(file_paths, output_file, variable)\n",
    "\n",
    "        #To use once spatial ensemble mean is calculated\n",
    "        print('Calculating the seasonal '+seas+' spatial ensemble mean')\n",
    "        calculate_seasonal_spatial_ensemble_mean_djf(output_file, variable, seas, output_file_djf, 1850, 2015)\n",
    "\n",
    "        #find the file for the ensemble spatial mean djf\n",
    "        folder_path = home+'ens_mean_spat/psl/'+e+'/'+m+'/'\n",
    "        file_path = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if seas in filename and period in filename][0]\n",
    "        output_file = home+'trend_calc_LESFMIP/linear_regression/NAO/'+e+'/'+m+'/'+variable+'_mon_'+e+'_'+m+'_'+seas+'_linear_trend_'+period+'_stats.nc'\n",
    "    \n",
    "        calculate_linear_trend_spat_pattern(file_path, variable, output_file)\n",
    "\n",
    "#now creating the NAO and EA regression maps\n",
    "for m in model:\n",
    "    #find the trend and crop to NA Sector, sort units.\n",
    "    trend_filename = home+'trend_calc_LESFMIP/linear_regression/NAO/historical/'+m+'/'+variable+'_mon_historical_'+m+'_'+seas+'_linear_trend_'+period+'_stats.nc'\n",
    "    trend = open_cropNA_unitshPA(trend_filename) * 165 #converts from Pa/year to hPa (the function converts from Pa to hPa so need to multiply by 165 for the hPa not hPa/year)\n",
    "\n",
    "    #path to the folder containing each experiment and model's psl anomalies (calculated seperatley)\n",
    "    folder_path = f'{home}psl_anomalies/{e}/{m}/'\n",
    "\n",
    "    #creating the list of files for this experiment and model's psl anomalies\n",
    "    ens_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if m in filename and '_EM' not in filename]\n",
    "\n",
    "    calculate_regression_map(ens_files, trend, 'NAO', 'slope', e, m, period)\n",
    "    calculate_regression_map(ens_files, trend, 'EA', 'slope', e, m, period)\n",
    "\n",
    "experiment = ['hist-aer','hist-GHG','hist-sol','hist-totalO3','hist-volc']\n",
    "\n",
    "#Now re-opening these maps and projecting the trends onto them\n",
    "for m in model:\n",
    "    print('Model:', m)\n",
    "    regression_NAO = xr.open_dataset(home+'regression_patterns/NAO/'+variable+'_mon_historical_'+m+'_'+seas+'_NAO_regression_map_'+period+'.nc')\n",
    "    regression_EA = xr.open_dataset(home+'regression_patterns/EA/'+variable+'_mon_historical_'+m+'_'+seas+'_EA_regression_map_'+period+'.nc')\n",
    "\n",
    "    for e in experiment:\n",
    "        print(\"Calculating the projection of the forced trend onto historical regression map calculated from all ensembles:\", e)\n",
    "        \n",
    "        #now setting up the folder path to get the file names for each experiment's model's forced response's trend\n",
    "        folder_path = home+'trend_calc_LESFMIP/linear_regression/NAO/'+e+'/'+m+'/'\n",
    "            \n",
    "        #Getting the list of file names within the models folder, should only be one trend per model (working off the ensemble means for each model)\n",
    "        ens_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if m in filename and period in filename]\n",
    "            \n",
    "        for i in range(0,len(ens_files)):\n",
    "            #cropping each trend to just the NA sector and whichever time\n",
    "            trend = open_cropNA_unitshPA(ens_files[i])#, 1850,2014)\n",
    "    \n",
    "            #multiplying the trend by 165 to convert to units of hPa (currently in units of hPa/year, trend calculated between 1850-2015)\n",
    "            trend = trend * 165\n",
    "\n",
    "            #calling the projection functions\n",
    "            proj_NAO, residual_NAO = project_onto_regression(trend, regression_NAO['regression_NAO_djf'], 'slope', 'NAO', e, m, period)\n",
    "            proj_EA, residual_EA = project_onto_regression(residual_NAO, regression_EA['regression_EA_djf'], 'slope', 'EA', e, m, period)\n",
    "\n",
    "#print(residual_NAO, residual_EA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fbbf5ad-fd17-4a66-9cef-a716a0c0f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the spatial ensemble means and trends for experiment e: historical\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n",
      "saved\n",
      "Calculating the seasonal DJF spatial ensemble mean\n",
      "in function\n",
      "saved file\n",
      "Calculating the spatial ensemble means and trends for experiment e: hist-aer\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n",
      "saved\n",
      "Calculating the seasonal DJF spatial ensemble mean\n",
      "in function\n",
      "saved file\n",
      "Calculating the spatial ensemble means and trends for experiment e: hist-GHG\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n",
      "saved\n",
      "Calculating the seasonal DJF spatial ensemble mean\n",
      "in function\n",
      "saved file\n",
      "Calculating the spatial ensemble means and trends for experiment e: hist-sol\n",
      "Model:  CanESM5\n",
      "Calculating the ensemble mean\n",
      "saved\n",
      "Calculating the seasonal DJF spatial ensemble mean\n",
      "in function\n",
      "saved file\n"
     ]
    }
   ],
   "source": [
    "#had to re-do these ones for the means.\n",
    "home = '/gws/nopw/j04/extant/users/slbennie/'\n",
    "\n",
    "\n",
    "variable = 'psl'\n",
    "period = '1950-2015'\n",
    "experiment = ['historical', 'hist-aer', 'hist-GHG', 'hist-sol']\n",
    "model = ['CanESM5']\n",
    "modes = ['NAO', 'EA']\n",
    "seas = 'DJF'\n",
    "\n",
    "for e in experiment:\n",
    "    print('Calculating the spatial ensemble means and trends for experiment e:', e)\n",
    "    for m in model:\n",
    "        print('Model: ',m)\n",
    "        #getting the LESFMIP file paths\n",
    "        folder_path = '/gws/nopw/j04/leader_epesc/CMIP6_SinglForcHistSimul/InterpolatedFlds/psl/'+e+'/'+m+'/'\n",
    "        file_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if e in filename and m in filename]\n",
    "\n",
    "        #creating output files for the mean across ensembles and the djf mean\n",
    "        output_file = home+'ens_mean_spat/psl/'+e+'/'+m+'/'+variable+'_mon_'+e+'_'+m+'_spatial_EM.nc'\n",
    "        output_file_djf = output_file.replace('spatial_EM', 'spatial_'+seas+'_EM_'+period)\n",
    "        \n",
    "        #find spatial mean across the ensembles\n",
    "        print('Calculating the ensemble mean')\n",
    "        calculate_spatial_ensemble_mean(file_paths, output_file, variable)\n",
    "\n",
    "        #To use once spatial ensemble mean is calculated\n",
    "        print('Calculating the seasonal '+seas+' spatial ensemble mean')\n",
    "        calculate_seasonal_spatial_ensemble_mean_djf(output_file, variable, seas, output_file_djf, 1850, 2015)\n",
    "\n",
    "        #find the file for the ensemble spatial mean djf\n",
    "        folder_path = home+'ens_mean_spat/psl/'+e+'/'+m+'/'\n",
    "        file_path = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if seas in filename and period in filename][0]\n",
    "        output_file = home+'trend_calc_LESFMIP/linear_regression/NAO/'+e+'/'+m+'/'+variable+'_mon_'+e+'_'+m+'_'+seas+'_linear_trend_'+period+'_stats.nc'\n",
    "    \n",
    "        calculate_linear_trend_spat_pattern(file_path, variable, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956da97-e2e1-43b7-be02-56eb92b971ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
